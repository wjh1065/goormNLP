{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HW6]DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c09ab1de5ca4bd58936b8e815094315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0485d4c8d3a74be3b54c0d3aa6d58bf2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2becc6354dc42ddbce5b7b879d48a76",
              "IPY_MODEL_c8fb4ebfa2ac4ac3a8e1e867d75cf51b",
              "IPY_MODEL_df0b3be517e2436b9cdde38289183144"
            ]
          }
        },
        "0485d4c8d3a74be3b54c0d3aa6d58bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2becc6354dc42ddbce5b7b879d48a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_44a795d2744241ff85d7ca8f46190678",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ed209d880484f518ea0f51880257c24"
          }
        },
        "c8fb4ebfa2ac4ac3a8e1e867d75cf51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d3d1c386f144b8e8f9df8a4cf3d8115",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96bb910e2a9746d3a58d8d7b8112afb8"
          }
        },
        "df0b3be517e2436b9cdde38289183144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ce6125b7edb43d0ae0bb282bb6bc67c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 33107313.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1315ec4c39d5454bb64a5c756374ec3f"
          }
        },
        "44a795d2744241ff85d7ca8f46190678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ed209d880484f518ea0f51880257c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d3d1c386f144b8e8f9df8a4cf3d8115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96bb910e2a9746d3a58d8d7b8112afb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ce6125b7edb43d0ae0bb282bb6bc67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1315ec4c39d5454bb64a5c756374ec3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a77d74739acc41ac9192c41a00e68253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_96f57ea9fa8747ae8fda2e370b0f2f97",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd4d1f9857ec4eb39c04b0eecb873b3b",
              "IPY_MODEL_c7e3f2ca361e422ea89b8250e66a472f",
              "IPY_MODEL_bf8b4425bdc34bc88e6ee3fb4429f97f"
            ]
          }
        },
        "96f57ea9fa8747ae8fda2e370b0f2f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd4d1f9857ec4eb39c04b0eecb873b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13736d97a20346c994845a328111ecae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e7ad83517054602b198974f59a065f8"
          }
        },
        "c7e3f2ca361e422ea89b8250e66a472f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db035842bfee4c9ea602e76c948bd27e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108949747,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108949747,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ab0f534430c4856aa316aacc075c81f"
          }
        },
        "bf8b4425bdc34bc88e6ee3fb4429f97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_329fd203b48d4b09acc9efd64404886d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:00&lt;00:00, 139MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09e2993dc9714623b954431ff7555c38"
          }
        },
        "13736d97a20346c994845a328111ecae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e7ad83517054602b198974f59a065f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db035842bfee4c9ea602e76c948bd27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ab0f534430c4856aa316aacc075c81f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "329fd203b48d4b09acc9efd64404886d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09e2993dc9714623b954431ff7555c38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR26RFkwXtvi"
      },
      "source": [
        "# **[HW6] DCGAN**\n",
        "1. DataLoader\n",
        "2. Model\n",
        "3. Inception Score\n",
        "4. Trainer\n",
        "5. Train\n",
        "\n",
        "이번 실습에서는 Convolution기반의 Generative Adversarial Network를 구현해서 이미지를 직접 생성해보는 실습을 진행해보겠습니다.\n",
        "\n",
        "- dataset: CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "- model: DCGAN (https://arxiv.org/abs/1511.06434)\n",
        "- evaluation: Inception Score (https://arxiv.org/abs/1801.01973)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crVJ36mMlaXP"
      },
      "source": [
        "\n",
        "\n",
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpvlE_XOWS33"
      },
      "source": [
        "런타임의 유형을 변경해줍니다.\n",
        "\n",
        "상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n",
        "\n",
        "변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqVdEuPQzMAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa388d8d-3c62-40f6-ef97-8b555d12cd09"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3-HPdHLZma"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import tqdm\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for reproducibility\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GnKJCB4T_Q"
      },
      "source": [
        "# 1. DataLoader\n",
        "\n",
        "이전의 실습들에서 사용한것과 마찬가지로, pre-defined된 CIFAR-10 dataset을 활용해서 dataloader를 만들어 두겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfV0OTc4Xdr"
      },
      "source": [
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def create_dataloader(batch_size=64, num_workers=1):\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, transform=transform, download=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data/', train=False, transform=transform, download=True)\n",
        "\n",
        "    trainloader = data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    testloader = data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1nhBnqWxw4a"
      },
      "source": [
        "# 2. Model\n",
        "\n",
        "이번 section에서는 DCGAN의 모델구조를 직접 구현해보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lycT_9vwaJN"
      },
      "source": [
        "우선 본격적인 모델 구현에 앞서 GAN의 전체적인 구조에 대해 살펴보겠습니다.\n",
        "\n",
        "GAN은 Generator와 Discriminator로 구성되어, Generator는 random latent vector를 받아 Discriminator를 속일 수 있는 fake image를 만들고, Discriminator는 real image와 fake image를 구분하는 형태로 학습이 진행되게 됩니다.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1mydINGwCR9maUffL-ejlT8vOPjWM5cYj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1h6nfvYwN8n"
      },
      "source": [
        "DCGAN은 image 데이터 처리에 효과적인 convolution layer를 활용하여 Generator와 Discriminator의 구조를 변형한 모델입니다.\n",
        "\n",
        "DCGAN의 Generator와 Discriminator의 구조는 아래와 같습니다.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1mp8jgDC5CDoZQNSGnq3kQRwSNQA7TIXl)\n",
        "\n",
        "\n",
        "이 때, Generator는 output의 width와 height를 키우는 convolution을 진행해주어야 하기 때문에, standard한 convolution operation이 아닌 deconvolution 혹은 transpose convolution이라고 불리는 연산을 통해 output의 size를 키워주는 연산을 진행하게 됩니다.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1mqoDvM3a4qnnu9IH60isrXtN7-RB_vgD)\n",
        "\n",
        "반대로, Discriminator는 Generator와 대칭되는 구조를 통해 standard한 convolution을 사용하여 classification을 진행해주게 됩니다.\n",
        "\n",
        "Transpose Convolution:(https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4t4Un1o2KVH"
      },
      "source": [
        "## Convolution Block\n",
        "\n",
        "우선, 모델을 쉽게 구현할 수 있도록, Generator와 Discriminator에서 반복적으로 사용할 convolution block고 deconvolution block을 정의해두도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssBO9DfqagW8"
      },
      "source": [
        "def conv(c_in, c_out, k_size, stride=2, pad=1, bias=False, norm='bn', activation=None):\n",
        "    layers = []\n",
        "\n",
        "    # Conv.\n",
        "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=bias))\n",
        "\n",
        "    # Normalization\n",
        "    if norm == 'bn':\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    elif norm == None:\n",
        "        pass\n",
        "\n",
        "    # Activation\n",
        "    if activation == 'lrelu':\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "    elif activation == 'relu':\n",
        "        layers.append(nn.ReLU())\n",
        "    elif activation == 'tanh':\n",
        "        layers.append(nn.Tanh())\n",
        "    elif activation == 'sigmoid':\n",
        "        layers.append(nn.Sigmoid())\n",
        "    elif activation == None:\n",
        "        pass\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def deconv(c_in, c_out, k_size, stride=2, pad=1, output_padding=0, bias=False, norm='bn', activation=None):\n",
        "    layers = []\n",
        "\n",
        "    # Deconv.\n",
        "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, output_padding, bias=bias))\n",
        "\n",
        "    # Normalization\n",
        "    if norm == 'bn':\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    elif norm == None:\n",
        "        pass\n",
        "\n",
        "    # Activation\n",
        "    if activation == 'lrelu':\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "    elif activation == 'relu':\n",
        "        layers.append(nn.ReLU())\n",
        "    elif activation == 'tanh':\n",
        "        layers.append(nn.Tanh())\n",
        "    elif activation == 'sigmoid':\n",
        "        layers.append(nn.Sigmoid())\n",
        "    elif activation == None:\n",
        "        pass\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYR2fBMM2kXM"
      },
      "source": [
        "## Generator\n",
        "\n",
        "이제, 위에서 정의한 deconv block을 활용해서 DCGAN의 Generator를 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDNAysVqxxOk"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        model = []\n",
        "\n",
        "        ### DCGAN Generator\n",
        "        # You have to implement 4-layers generator.\n",
        "        # Note: Recommend to use 'deconv' function\n",
        "        ### YOUR CODE HERE (~ 4 lines)\n",
        "        model += [deconv(256, 256, 4, 1, 0, norm='bn', activation='relu'),\n",
        "                  deconv(256, 128, 4, norm='bn', activation='relu'),\n",
        "                  deconv(128, 64, 4, norm='bn', activation='relu'),\n",
        "                  deconv(64, 3, 4, norm=None, activation='tanh')]\n",
        "        ### END YOUR CODE\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Input (z) size : [Batch, 256, 1, 1]\n",
        "        # Output (Image) size : [Batch, 3, 32, 32]\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        output = self.model(z)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DxU-78B33dG"
      },
      "source": [
        "## Discriminator\n",
        "\n",
        "이제, 위에서 정의한 conv block을 활용해서 DCGAN의 Discriminator를 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0U2s0hux_n6"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        model = []\n",
        "\n",
        "        ### DCGAN Discriminator\n",
        "        # You have to implement 4-layers discriminator.\n",
        "        # Note: Recommend to use 'conv' function \n",
        "        ### YOUR CODE HERE (~ 4 lines)\n",
        "        model += [conv(3, 64, 4, norm='bn', activation='lrelu'),\n",
        "                  conv(64, 128, 4, norm='bn', activation='lrelu'),\n",
        "                  conv(128, 256, 4, norm='bn', activation='lrelu'),\n",
        "                  conv(256, 1, 4, 1, 0, norm=None, activation='sigmoid')]\n",
        "        ### END YOUR CODE\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Input (z) size : [Batch, 3, 32, 32]\n",
        "        # Output (probability) size : [Batch, 1]\n",
        "        output = self.model(x).squeeze()\n",
        "\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W68n93Mb6aMi"
      },
      "source": [
        "## Implementation Test\n",
        "\n",
        "이제 Generator와 Discriminator를 맞게 구현했는지 test해보도록 하겠습니다.\n",
        "\n",
        "체크를 위해서 코드와 함께 주어졌던 두개의 파일\n",
        "- sanity_check_dcgan_netG.pth\n",
        "- sanity_check_dcgan_netD.pth\n",
        "\n",
        "를 왼쪽 상단에 [파일]->[세션 저장소에 업로드]를 눌러 업로드 하고, \\\\\n",
        "아래의 코드를 실행시켜 코드가 통과되면 성공입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8SnkmI95Tvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4fecfb-0ca5-4bd0-c16f-d7954e7346fa"
      },
      "source": [
        "def test_model():\n",
        "    print(\"=====Model Initializer Test Case======\")\n",
        "    netG = Generator()\n",
        "    # the first test\n",
        "    try:\n",
        "        netG.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_CS/sanity_check_dcgan_netG.pth\", map_location='cpu'))\n",
        "    except Exception as e:\n",
        "        print(\"Your DCGAN generator initializer is wrong. Check the comments in details and implement the model precisely.\")\n",
        "        raise e\n",
        "    print(\"The first test passed!\")\n",
        "\n",
        "    # the second test\n",
        "    netD = Discriminator()\n",
        "    try:\n",
        "        netD.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_CS/sanity_check_dcgan_netD.pth\", map_location='cpu'))\n",
        "    except Exception as e:\n",
        "        print(\"Your DCGAN discriminator initializer is wrong. Check the comments in details and implement the model precisely.\")\n",
        "        raise e\n",
        "    print(\"The second test passed!\")\n",
        "    print(\"All 2 tests passed!\")\n",
        "\n",
        "test_model()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Model Initializer Test Case======\n",
            "The first test passed!\n",
            "The second test passed!\n",
            "All 2 tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwpv64KktIeI",
        "outputId": "81632756-df36-4d35-89d9-2ab01bb71c9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36LwdYMDatum"
      },
      "source": [
        "# 3. Inception Score\n",
        "\n",
        "비록 이제 dataloader와 model을 구현하였지만, 본격적으로 학습을 진행하기전 지도학습과 다르게 한가지 추가적으로 필요한 것이 있습니다.\n",
        "\n",
        "기존의 지도학습 세팅에서는 loss나 validation accuracy를 통해서 학습이 원활히 진행되고 있는지 모니터링이 가능했지만, GAN에서는 generator가 비록 discriminator를 잘 속이고 있을지라도 (i.e., 낮은 loss) discriminator가 학습이 충분히 되지 못했다면 낮은 퀄리티의 이미지가 생성되게 됩니다.\n",
        "\n",
        "이미지의 퀄리티를 측정하는 방법은 크게 2가지 입니다.\n",
        "1. Fidelity(충실도): 얼마나 고품질의 이미지를 생성하는가?.\n",
        "2. Diversity(다양성): 생성된 이미지들이 얼마나 다양한가? (e.g., 고양이만 생성하지 않음)\n",
        "\n",
        "보통 Fidelity를 측정하기 위해서는 **Frechet Inception Distance**라는 metric이, Diversity를 측정하기 위해서는 **Inception Score**라는 evaluation metric이 사용되곤 합니다.\n",
        "\n",
        "이번 실습에서는 이미지의 다양성을 측정하는 Inception Score를 통해 학습이 원활히 진행되고 있는지 모니터링 하도록 하겠습니다.\n",
        "\n",
        "Inception score를 측정하는 방법은 아래와 같습니다.\n",
        "1. Generator를 통해 이미지를 N개 생성한다.\n",
        "2. 생성된 이미지들을 pre-trained된 inception network (=googleNet)에 통과시킨다.\n",
        "3. inception network가 예측한 생성된 image의 label별 probability의 평균이 얼마나 diverse한지 측정한다.\n",
        "\n",
        "Inception score에 대한 자세한 내용이 궁금하신 분은 아래를 참조해주세요\n",
        "- https://arxiv.org/abs/1801.01973\n",
        "- https://cyc1am3n.github.io/2020/03/01/is_fid.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QRDNecSHTLz"
      },
      "source": [
        "from torchvision.models.inception import inception_v3\n",
        "from scipy.stats import entropy\n",
        "\n",
        "class Inception_Score():\n",
        "    def __init__(self, dataset):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Dataset & DataLoader\n",
        "        self.N = len(dataset)\n",
        "        self.batch_size = 64\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.dataloader = data.DataLoader(dataset=dataset, batch_size=self.batch_size, num_workers=1)\n",
        "        self.transform = nn.Upsample(size=(299, 299), mode='bilinear').to(self.device)\n",
        "\n",
        "        # Inception Model\n",
        "        self.inception_model = inception_v3(pretrained=True, transform_input=False).to(self.device)\n",
        "        self.inception_model.eval()\n",
        "\n",
        "    def get_pred(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.transform(x)\n",
        "            x = self.inception_model(x)\n",
        "            return F.softmax(x, dim=1).data.cpu().numpy()\n",
        "\n",
        "    def compute_score(self, splits=1):\n",
        "        preds = np.zeros((self.N, 1000))\n",
        "\n",
        "        for i, batch in tqdm.tqdm(enumerate(self.dataloader)):\n",
        "            batch = batch.to(self.device)\n",
        "            batch_size_i = batch.size(0)\n",
        "            preds[i * self.batch_size : i * self.batch_size + batch_size_i] = self.get_pred(batch)\n",
        "\n",
        "        # Compute the mean KL-divergence\n",
        "        # You have to calculate the inception score.\n",
        "        # The logit values from inception model are already stored in 'preds'.\n",
        "        inception_score = 0.0\n",
        "        split_scores = []\n",
        "        for k in tqdm.tqdm(range(splits)):\n",
        "            part = preds[k * (self.N // splits): (k + 1) * (self.N // splits), :]\n",
        "            py = np.mean(part, axis=0)\n",
        "            scores = []\n",
        "            for i in range(part.shape[0]):\n",
        "                pyx = part[i, :]\n",
        "                scores.append(entropy(pyx, py))\n",
        "            split_scores.append(np.exp(np.mean(scores)))\n",
        "        inception_score = np.mean(split_scores)\n",
        "\n",
        "        return inception_score"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtzK6bKsHfaj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271,
          "referenced_widgets": [
            "0c09ab1de5ca4bd58936b8e815094315",
            "0485d4c8d3a74be3b54c0d3aa6d58bf2",
            "e2becc6354dc42ddbce5b7b879d48a76",
            "c8fb4ebfa2ac4ac3a8e1e867d75cf51b",
            "df0b3be517e2436b9cdde38289183144",
            "44a795d2744241ff85d7ca8f46190678",
            "4ed209d880484f518ea0f51880257c24",
            "0d3d1c386f144b8e8f9df8a4cf3d8115",
            "96bb910e2a9746d3a58d8d7b8112afb8",
            "3ce6125b7edb43d0ae0bb282bb6bc67c",
            "1315ec4c39d5454bb64a5c756374ec3f",
            "a77d74739acc41ac9192c41a00e68253",
            "96f57ea9fa8747ae8fda2e370b0f2f97",
            "cd4d1f9857ec4eb39c04b0eecb873b3b",
            "c7e3f2ca361e422ea89b8250e66a472f",
            "bf8b4425bdc34bc88e6ee3fb4429f97f",
            "13736d97a20346c994845a328111ecae",
            "3e7ad83517054602b198974f59a065f8",
            "db035842bfee4c9ea602e76c948bd27e",
            "6ab0f534430c4856aa316aacc075c81f",
            "329fd203b48d4b09acc9efd64404886d",
            "09e2993dc9714623b954431ff7555c38"
          ]
        },
        "outputId": "baccb4fc-31b7-42e0-aaf3-ded30224f7cd"
      },
      "source": [
        "def test_inception_score():\n",
        "    print(\"======Inception Score Test Case======\")\n",
        "\n",
        "    # CIFAR10 Datset without Label\n",
        "    class CIFAR10woLabel():\n",
        "        def __init__(self):\n",
        "            transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
        "            self.dataset = torchvision.datasets.CIFAR10(root='./data/', download=True, transform=transform)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return self.dataset[index][0]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "    print(\"Calculating Inception Score...\")\n",
        "\n",
        "    Inception = Inception_Score(CIFAR10woLabel())\n",
        "    score = Inception.compute_score(splits=1)\n",
        "\n",
        "    assert np.allclose(score, 9.719672, atol=1e-3), \\\n",
        "        \"Your inception score does not match expected result.\"\n",
        "\n",
        "    print(\"All test passed!\")\n",
        "\n",
        "test_inception_score()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Inception Score Test Case======\n",
            "Calculating Inception Score...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c09ab1de5ca4bd58936b8e815094315",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a77d74739acc41ac9192c41a00e68253",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/104M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "782it [10:14,  1.27it/s]\n",
            "100%|██████████| 1/1 [00:03<00:00,  3.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test passed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa2ZABccII_K"
      },
      "source": [
        "# 4. Trainer\n",
        "\n",
        "이제 앞서 선언한 dataloader, model, evaluator를 모두 활용해서 GAN을 학습시키는 Trainer를 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2PIMmgCYQU7"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "\\begin{equation}\n",
        "D_{\\theta}: \\text{Discriminator network}\\\\\n",
        "G_{\\phi}: \\text{Generator network}\\\\\n",
        "x: \\text{real_image} \\\\\n",
        "z: \\text{latent_vector} \\\\\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18SBMREkcjZh"
      },
      "source": [
        "## Discriminator Loss\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{D_{\\theta}} = -E_{x \\sim p_{data}}[logD_{\\theta}(x) + E_{z}[\\log(1 - D_{\\theta}(G_{\\phi}(z)))]]\n",
        "\\end{equation}\n",
        "\n",
        "Discriminator loss는 위와 같이 real_image는 1으로, generated_image는 0으로 판별하는 방식으로 학습을 진행하게 됩니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIHlU8M0cpPy"
      },
      "source": [
        "## Generator Loss\n",
        "\n",
        "Generator network는 이론적으로는 discriminator의 loss에서 generator가 해당되는 부분에 -1을 곱해서 표현할 수 있습니다.\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{G_{\\phi}} = E_{z}[\\log(1-D_{\\theta}(G_{\\phi}(z))]  \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "하지만, 위의 식으로 학습을 진행할 경우 Generator의 학습이 원활히 이루어지지 않게되는 문제점이 있습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0ImQBI_8oEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0b428ec1-f415-4816-adb5-8737f5006af0"
      },
      "source": [
        "plt.title('log(1-D(G(z))')\n",
        "x = np.arange(0, 1.0, 0.01)\n",
        "y = np.log(1-x)\n",
        "plt.xlabel('D(G(z))')\n",
        "plt.plot(x,y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f970a48e350>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEWCAYAAAB7QRxFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxU1Z338c+veqV3et9pmmYHQWgQiYpGRU2imLjE0ZiYxwRN4kzmyeIk40zizCTzTCb7NlEnmzExcUniRqLGDbeAArLv0NBsvULTG9Dbef6oghRtQzd0Vd2u7u/79aoXfZeq+zs2fj2eOvdcc84hIiLRy+d1ASIiMjgKchGRKKcgFxGJcgpyEZEopyAXEYlyCnIRkSinIBdPmNkuM7ssBJ9zhZk9EYqa+rnOHWb2vbN43/8zs3/s55wEM9tsZjlB+642s0fOplYZeRTkEu2+DvzX8Q0z+w8zW2dmXWZ2b39vNjNnZm1m1mpmjWb2opl9uNc58cC/AN8M3mdmXzGzLYH37zOzP5vZwqBzcoCPAvefrgbn3DHg58CXgvY9DUw1s3P6a4OIglyilpnNAdKdc8uCdm8H7gaWnMFHzXDOpQATgV8CPzKzrwYdXwRsds7tC9r3eGD/R4HRwFjg+8D7g865DfiTc+7IAGp4GPiYmSUE7fstsPgM2iEjlIJcPBUYVvieme0PvL4XHGZmdreZHQgc+0SgB10ROHwVsDT485xzDzrn/gy0nGktzrkG59xDwKeAL5tZVl/XCQwJXQ4scs4td851BF7POuc+G/SRvd/3dKDnf/zVY2a3Ba69FzgEzAt6/yuc/B8GkT4pyMVr9+APr5nADGAu/mEMzOxK4HPAZUAFcHGv904HtoShpieB2EAtfV3nMmB5IHxP56T3Oeeuds6lBHr/NwA1wItB52/C/88geLvMzNLOqhUyYijIxWu3AP/unKtzztUD/wbcGjh2I/AL59wG51w7cG+v92ZwFj3v/jjnOoEGIPMU18nGH8IAmFmmmTWZ2WEzO9pffWY2AXgQuNE5tyfoUEvgPcHb9Non8i4KcvFaIbA7aHt3YN/xY8FBF/wz+IciUgd6ITPbEDSsceFpzosDcoCDp7hOI1BwfMM5d9A5lwHMBoLHuN9Vn5ml4+/x/4tz7vVel04Fmnpt02ufyLsoyMVr+4ExQdulgX0AB4DioGMlvd67Fpgw0As556YeH9pwzr12mlMXAV3AW6e4zovAHDMr7v3G09VnZj78X2q+7Jx7oI/zJwNrem3vcs4193MdGeEU5OK13wL/YmY5ZpYNfAX4deDYo8DHzWyymSUB/9rrvX8CFgTvMLM4M0vE/3c71swSzSxmIIUEhkhuAX4MfMM519jXdZxzzwMvA0+Y2XmBqYhxnPxFZV/1fR1IBj7b6zzMrAj/UE7wDJwFwJ8HUruMcM45vfSK+AvYhf9Lw0TgB/h73wcCPycGnfdl/OPR+/HPJnFASdDxt4HzgrZ/GTgn+HXbaepwQBvQin8o5WXg5l7nxAHVQGHQvnj8Y/bbgHZgL/7QXRh0TnZg/6igNh8NXOv465bAsS8C3+l13XX4p0Z6/vvSa2i/zDk9WEKig5lNBtYDCc65rsC+hcCnnXPXhvnai4EpzrnT3qXZx/v+E6hzzp3yrtDAdMs1wEXOubrAvquBW51zNw6ibBkhFOQypJnZB/EPUSThn+nRE+7QFok2GiOXoe4OoA7YAXTjH14RkSDqkYuIRDn1yEVEolysFxfNzs52ZWVlXlxaRCRqrVy5ssE5l9N7vydBXlZWxooVK7y4tIhI1DKz3X3t19CKiEiUU5CLiEQ5BbmISJRTkIuIRDkFuYhIlAtJkJvZlYGH0G43sy/1/w4REQmVQQd5YInQH+N/PuEU4O/MbMpgP1dERAYmFPPI5wLbnXM7Aczsd/gX5t8Ygs8+yYubatlS20JZVjJjspIYk5VMSoInU+FFRIaMUKRgESc/gmsvcF7vkwLLgC4GKC0tPasLLd1az6/+evJ8+OyUBMoCoT42+/ifyZRlK+RFZGQY9KJZZnY9cKVz7hOB7VvxL/R/16neU1lZ6c72zs7WY13sbmxjd2M7VQ1tVDe2s6uxjV2NbdQ2Hzvp3OyUBMZmJ50I9vLAn2VZySTGDeihMSIiQ4aZrXTOVfbeH4ou6z5OfpZicWBfWKQkxDK1MJ2phenvOtbe0cXuxnZ2NbRR1djm/7OhjZc219PQuvfEeWZQmD6Ksdn+3nt5jv/PcTkpFGaMIsZn4SpfRCTkQhHkbwPjzWws/gC/Cbg5BJ97xpLiY5lckMbkgrR3HWs52smuhnaqGtuoqm+jqqGVqoY2nnhnHy3Huk6cFx/roywrifLsFMpz/OFenpNMeU4K6aPiItkcEZEBGXSQO+e6zOwu4DkgBvi5c27DoCsLsdTEOKYXpzO9+OSevHOOhtYOqhr84b6zvo0d9W1srWvhhU21dPX8begpOyWBcTnJjMtNYVxOChW5KYzLSaYwfRQ+9eJFxCMh+TbQOfcn/I/jijpmRk5qAjmpCcwdm3nSsc7uHqoPtrOzvo3tda3srG9lR30rz6zZT/PRv/XiR8XFUJ6TTEVuChU5KYzP84f8mKxk4mJ0z5WIhJemdZxGXIyPcTn+3vflU/JO7HfO0djWwY66VrbXt7K9rpUd9W2s2HWIJ1fvP3FerM8Ym+0P+PG5KYzPS2V8Xgpjs5NJiNWXrSISGgrys2BmZKckkJ2SwHnlWScdazvWxc76NrbVtbCtzh/ym2taeG5DDcdHaWJ8xpisJCbkpjIhzx/wE/JSGZudTHysevAicmYU5CGWnBDb51j80c5uqhra2FrbwrbaVrbVtbC1roXnN/4t4GN9RnlOMuPzUpkYCPdJ+amUZCZpJo2InJKCPEIS42L6nFFzrKubnfX+gN9S08LW2lbW7T3MkrUHgt7rY3yuP9Qn5qcyKT+NSQWpZKckRLoZIjIEKcg9lhDbd8C3d3SxrbaVLTUtbK5pYUttMy9vqeOxlX+bD5+dEu8P9fxUJhX4/xyfl6Lxd5ERRkE+RCXFxzKjJIMZJRkn7W9oPXYi3DcdaGZLTQsPLdvNsa4ewD88My4nhckFqUwuSGNKof8/Euq9iwxfCvIok52SQHZFAu+pyD6xr7vHUdXQxuaaZjYdaGbTgRaWVx3kiaAZNLmpCUwpTGNKINynFqYzJjNJ899FhgEF+TAQ4zP/HPbcFD5wTuGJ/YfaOth0oJmNx1/7m3l9W8OJm5xSEmJPBPu0onSmFqZRkZuiue8iUWbQi2adjcEsmiWDc7Szm+11rWzYf5gN+5vZsN8f8Ec6uwH/EgWT81OZWpTO9MBrQl6qpkWKDAHhXDRLokhiXAzTitKZVvS36ZH+oZlWNuxvZv2+w6zbd5in1+zn4eXVAMTH+JhUkMr0onTOKU5nelEG4/PUcxcZKtQjlz719DiqD7azbt/hE+G+bt9hWgJLEyTE+phSmMaM4gzOKU5nRkkGY7OSNeYuEkan6pEryGXAenocuw+2s3ZvE2v3HmbdXn+4Hx+WSU2MZUZxBjNK0plRnMHM0gxyUxM9rlpk+NDQigyaL7B2zNjsZBbNLAKgq7uH7fWtrN1zmNV7m1izp4n7lu6kO/CFalHGKGaWZnBuSQbnlmYwtTBdD/UQCTH1yCXkjnR0s2H/YVbvaeKdPU2srm5iX9MRAOJijCmF6ZxbksGsMaOZVZpBUcYozDQkI9IfDa2Ip+pajrK6uolV1U2sqj7E2r1NHO3038SUl5bA7DGjmVU6msqyTKYWpumLVJE+aGhFPJWbmsjCqfksnJoP+Nd633yghVXVh1hVfYiVuw/xp3U1gH9tmRnFGcwpy2R2mT/g9XQmkVNTj1yGjNrmo6zcfYi3dx1k5e5DbNjfTHePwwwm5qUyd2wmc8oymTs2k7w0fYkqI4+GViTqtB3rYs2eJt7e5Q/3VdWHaO/wz5Apy0pi7thM5o7N4ryxmZRkJnlcrUj4aWhFok5yQizzK7KZH1hXprO7h437m3mr6iDLqw7y3IZaHl3hXw2yKGMU55VnMq88i/PLsxTsMqKoRy5Rq6fHsaW2heU7G1keCPeDbR0AFI8exbzyLOaPy+L8cVkUpI/yuFqRwdPQigx7PT2ObXWtLNvZyF93NLKsqpGm9k4AyrOTOX9cFu+pyGb+uCwykuI9rlbkzCnIZcTp6XFsqmnmrzsaeXNHI8t3NtLW0Y0ZTC9K5z0V2VxQkc3sMaN1k5JEBQW5jHid3T2s3dvE69saeWN7A6uqD9HV40iM8zF3bBYXjc/mogk5jM9N0Q1KMiQpyEV6aT3WxfKdjby2rYFXt9Wzs74NgPy0RC6akM2CCblcUJFNepLmsMvQoCAX6ce+piO8trWeV7fV89q2BlqOduEzmFU6mksm5XLxxBymFKSpty6eUZCLnIGu7h5W72nilS31LN1az7p9hwH/I/PeOymXSyb5e+vJCZrBK5GjIBcZhLqWoyzdUs8rW+p5dWs9Lce6iI/xcf64LC6bnMulk/MozNAURwkvBblIiHR29/D2roO8tKmOFzfXUdXgH1ufUpDG5VPyuHxKHlMLNQQjoacgFwmTHfWtvLCxlhc21bJi9yGc899pevmUPK6Yms+cstHEajVHCQEFuUgENLYe48VNdTy/sYZXtzXQ0dVDZnI8l0/O48pp+bynIlsPspazpiAXibC2Y10s3VrPcxtqeGlTHS3HukhNjOXyyXlcNb2AC8dn60YkOSMKchEPHevq5o3tDfx5XQ3Pb6zl8JFOUhNiuXxKHu8/p4ALx+eopy79UpCLDBGd3T28uaORJWv38+z6GpqPdpGWGMtV0wq4ekYh88ozNaYufVKQiwxBHV09vL69nmfWHOD5jbW0HusiOyWBq2cUcO3MIs4pTtfsFzkhLEFuZjcA9wKTgbnOuQGls4Jc5N2Odnbz8uY6nly9n5c219HR3UN5djLXnlvEB88t0hrrErYgnwz0APcDX1CQi4TG4SOdPLv+AH98Zx/Ldh4EYG5ZJtfNLuJ90wtITdT6LyNRWIdWzOwVFOQiYbGv6QhPvLOP36/ay876NhLjfFw1rYAbKouZNzYLn09DLyOF50FuZouBxQClpaWzd+/ePejriowkzjlW72ni8ZV7eWrNflqOdlGSOYobZ5dwQ2UJ+el6IPVwd9ZBbmYvAPl9HLrHOfdk4JxXUI9cJGKOdnbz3IYaHnl7D2/uaMRncMnEXG6aW8olE3M062WYOuuHLzvnLgtPSSJythLjYlg0s4hFM4vY3djGI2/v4bGVe3nxVysoSE/kw3NKuGlOqXrpI4TGyEWGic7uHl7cVMdvlu/mtW0NxPiMhVPyuPX8MZxfnqVpjMNAuGatfBD4IZADNAGrnXNX9Pc+BblIeO1ubOPh5dU8smIPTe2djM9N4WPzy/jQrCKS4rWGerTSDUEiI9DRzm6eXrOfB/+6i/X7mklNjOWmOSV8bH4ZxaM1Lz3aKMhFRjDnHKuqD/HzN3bx7PoaAK6cms/tF45lVuloj6uTgTrrLztFJPqZGbPHZDJ7TCb7m47w4Ju7ePitapasO8CcstEsvmgcl07K1Zz0KKUeucgI1Xasi0dX7OGnr1Wxr+kI43KSuWPBOK6dWaSVGIcoDa2ISJ+6untYsu4A9y/dycYDzRSmJ/LJi8q5aU4po+K1XvpQoiAXkdNyzvHK1np+8vIO3tp1kOyUeBZfVM4t540hOUGjsEOBglxEBuytqoP88KVtvLatgcxkf6B/9PwxmrroMQW5iJyxVdWH+P4L21i6tZ7slHjuXDCOj8wbo0fUeURBLiJnbeXug3z3L9t4fXsDeWkJ/MOl47mxsoQ4rekSUacKcv0WRKRfs8dk8utPnMfvFs+jeHQS9/xxPZd/ZynPrN2PF51BOZmCXEQGbF55Fo/feT4/+1gliXEx3PXwO1z7P2+ybGej16WNaApyETkjZsalk/NY8g8X8q0bZlDXfJSbHljGJ3+1gqqGNq/LG5EU5CJyVmJ8xvWzi3n5CxfzxSsm8ub2BhZ+dylfe2Yjh490el3eiKIgF5FBSYyL4TOXVPDyFy/mQ+cW87M3qrj026/w2Io99PRo/DwSFOQiEhK5qYl84/pzePquCyjNTOKLj6/luvveZP2+w16XNuwpyEUkpKYVpfP4nfP59g0z2HPwCNf86HX+/emNtB7r8rq0YUtBLiIh5/MZ180u5sXPL+Dm80r5xZtVXPbtpTy3ocbr0oYlBbmIhE36qDi+du10fv+p+WQkxXHHQyv5zG9WUd9yzOvShhUFuYiE3azS0Tz99xfwxSsm8peNtVz2naX8YdVe3UwUIgpyEYmIuBgfn7mkgj999kIqclP43KNruOOhlTS0qnc+WApyEYmoitwUHr3jfP75fZN4ZUs9C7/7Ks+uP+B1WVFNQS4iERfjMxZfNI5n/uECijJGceevV/FPj6+lvUMzW86GglxEPDMhL5U/fHo+n754HI+u3MMHfvA66/Zq3vmZUpCLiKfiYnzcfeUkHv7EPNo7uvnQT97gl29U6YvQM6AgF5Eh4fxxWTz7jxeyYEIO9z69kc88vIrmo1qzZSAU5CIyZGQkxfO/H63kn983iec21HL1D19nc02z12UNeQpyERlSzPxfhD6yeB5HOrr50P+8yZK1mtVyOgpyERmSKssyeebvL2ByQRqfeXgV//XnzXRrNcU+KchFZMjKTUvkt5+cx83nlXLf0h3c8dBK2rT41rsoyEVkSIuP9fGfH5zOv10zlZc213LDfX/lwOEjXpc1pCjIRSQqfGx+GT+/bQ7VB9tZ9KM32LBf882PU5CLSNS4eGIuv//UfGJ9xofvX8abOxq8LmlIUJCLSFSZmJ/K7z89n8KMRG77+dua0YKCXESiUEH6KB67Yz4zStK567er+O1b1V6X5CkFuYhEpfSkOB66/TwumZjLl/+wjl+8UeV1SZ4ZVJCb2TfNbLOZrTWzP5pZRqgKExHpT2JcDPd9ZDZXTs3n357eyE9e2eF1SZ4YbI/8L8A059w5wFbgy4MvSURk4OJjffzo5nNZNLOQbzy7mR+/vN3rkiIudjBvds49H7S5DLh+cOWIiJy52Bgf37lxJgZ887ktJMT6+MSF5V6XFTGDCvJe/g/wyKkOmtliYDFAaWlpCC8rIuJ/WMW3bphBR3cPX1uyiYS4GG6dN8brsiKi3yA3sxeA/D4O3eOcezJwzj1AF/CbU32Oc+4B4AGAyspKLZggIiEXG+Pj+zedS0fXSv71ifUkx8fwoVnFXpcVdv0GuXPustMdN7PbgA8AlzqtBC8iHouL8fHjW2bx8V+8zd2PryUrJYEFE3K8LiusBjtr5UrgbuAa51x7aEoSERmchNgY7r91NuPzUvnUr1cO+8fHDXbWyo+AVOAvZrbazO4LQU0iIoOWmhjHgx+fw+ikeD7+y7fY3djmdUlhM6ggd85VOOdKnHMzA687Q1WYiMhg5aYl8qvb59LV47j9wRW0DNNHx+nOThEZ1sblpPA/t8xiV0Mbn/3d6mH5cAoFuYgMe/PHZfPVa6by0uY6/vu5zV6XE3KhnEcuIjJk3TpvDJsPNHP/0p1Mzk/j2nOLvC4pZNQjF5ER495rpjK3LJMv/2Ed2+tavC4nZBTkIjJixMX4+OHN55IUH8NnfvMORzq6vS4pJBTkIjKi5KUl8t0Pz2RrXQtffWq91+WEhIJcREaciybkcNclFTy6Yi9/WLXX63IGTUEuIiPSZy8dz9yyTL765Ab2Nx3xupxBUZCLyIgUG+Pj2zfOoNs57n58LdG8VJSCXERGrJLMJO55/2Re397Ar5dH73M/FeQiMqLdPLeUC8dn859LNrGrITrXY1GQi8iIZmb89/XnEBtj/NPvo3OIRUEuIiNeQfoovnzVZJZXHeSJ1fu8LueMKchFRICb5pQwoySDry/ZxOEj0bVKooJcRATw+YyvXzuNg20dfOf5LV6Xc0YU5CIiAdOK0rl13hgeWrY7qp4qpCAXEQnyuYUTyUxO4CtPrY+aLz4V5CIiQdJHxfGFhRN4p7qJ5zbUel3OgCjIRUR6uX52MeNykvnmc5vp6u7xupx+KchFRHqJjfHxxSsmsqO+jd9HwaJaCnIRkT5cMTWfmSUZfPcv2zjaObTXLVeQi4j0wcz40lWTqGk+yi/f3OV1OaelIBcROYV55VlcPDGH+5buoL2jy+tyTklBLiJyGnddUkFTeyePvL3H61JOSUEuInIalWWZVI4ZzU9fq6JziM5gUZCLiPTjzgXj2Nd0hGfW7ve6lD4pyEVE+vHeSbmMz03h/qU7h+TdngpyEZF++HzGHQvGsbmmhVe21HtdzrsoyEVEBuCaGYUUpCdy39IdXpfyLgpyEZEBiI/1cdv8MpZXHWRbbYvX5ZxEQS4iMkDXzS4m1mf8bohNRVSQi4gMUHZKApdPyeMPq/ZyrGvo3LavIBcROQM3zS3lUHsnzw+hJW4V5CIiZ+DCimyKMkYNqTs9BxXkZvYfZrbWzFab2fNmVhiqwkREhiKfz7ixsoTXtzdQ3djudTnA4Hvk33TOneOcmwk8A3wlBDWJiAxpN1QW4zN4dMXQ6JUPKsidc81Bm8nA0LvlSUQkxAozRrFgQg6PrdxDd4/3sTfoMXIz+7qZ7QFu4TQ9cjNbbGYrzGxFff3QuzNKRORMfHBWMbXNx1hVfcjrUvoPcjN7wczW9/FaBOCcu8c5VwL8BrjrVJ/jnHvAOVfpnKvMyckJXQtERDzw3km5xMf4eHZ9jdel9B/kzrnLnHPT+ng92evU3wDXhadMEZGhJSUhlgvHZ/Ps+hrPF9Ia7KyV8UGbi4DNgytHRCR6XDEtn31NR1i/r7n/k8ModpDv/y8zmwj0ALuBOwdfkohIdLhsch4xPuPZDQeYXpzuWR2DnbVyXWCY5Rzn3NXOuX2hKkxEZKjLTI7nvLGZno+T685OEZFBuHJaPjvq29he592KiApyEZFBWDglH8DTXrmCXERkEPLTE5lVmsGzGxTkIiJRa+HUfNbva6bm8FFPrq8gFxEZpAsqsgFYXtXoyfUV5CIigzS5II3UxFiW7VSQi4hEpRifMbcsk2U7D3pyfQW5iEgIzCvPoqqhjdrmyI+TK8hFREJgXnkWgCfDKwpyEZEQmFKYRmpCrCfDKwpyEZEQiPEZc8ZmejJzRUEuIhIi88oz2VnfRl2Ex8kV5CIiIXJinLwqssMrCnIRkRCZUpBGSkLk55MryEVEQiQ2xsecstEsV5CLiESveeVZ7Khvo77lWMSuqSAXEQmhc4ozANh4IHKPf1OQi4iE0KT8VAC21CjIRUSi0ujkeHJTE9hcE7knBinIRURCbGJ+KlsU5CIi0WtyQRrb6lrp6u6JyPUU5CIiITYxL5WOrh52NbZH5HoKchGREJsY+MJzc4S+8FSQi4iEWEVuCjE+i9g4uYJcRCTEEuNiGJudHLGZKwpyEZEwiOTMFQW5iEgYTMpLpfpgO23HusJ+LQW5iEgYHP/Cc0tt+HvlCnIRkTCYXJAGEJHhFQW5iEgYFGWMIjk+RkEuIhKtfD5jQn5qROaSK8hFRMJkUmDminMurNdRkIuIhMnEvFQOtXdSF+aHTCjIRUTCpCLXP3OlqqEtrNcJSZCb2efNzJlZdig+T0RkOMhPTwCgtvloWK8z6CA3sxJgIVA9+HJERIaP3LREAOqah/7QyneBu4HwjuaLiESZ1IRYkuJjqBnKPXIzWwTsc86tGcC5i81shZmtqK+vH8xlRUSigpmRl5YY9qGV2AEU8gKQ38ehe4B/xj+s0i/n3APAAwCVlZXqvYvIiJCXluB9kDvnLutrv5lNB8YCa8wMoBhYZWZznXM1Ia1SRCRK5aUl8k51U1iv0W+Qn4pzbh2Qe3zbzHYBlc65hhDUJSIyLOSnJVLTfBTnHIFOb8hpHrmISBjlpiXS0dXD4SOdYbtGyILcOVem3riIyMnyA1MQwzlzRT1yEZEwyks7flNQ+OaSK8hFRMIoL9Ajrz2sHrmISFTKTQv/bfoKchGRMEqIjWF0UpzGyEVEopn/7k6NkYuIRK28tETqWtQjFxGJWvlpidToy04RkeiVl5ZAQ+sxurp7wvL5CnIRkTDLS0+kx0FDa0dYPl9BLiISZnmpgbnkYZq5oiAXEQmzvDDfpq8gFxEJs7zAszvrFOQiItEpKzmBGJ+pRy4iEq1ifEZuakLYbgpSkIuIREBuGJ/dqSAXEYmA/DA+u1NBLiISAXlhvLtTQS4iEgF5aYk0H+3iSEd3yD9bQS4iEgEnHjARhuEVBbmISATkK8hFRKLbmKwkrpqWT1J8bMg/O/SfKCIi71KSmcRPPjI7LJ+tHrmISJRTkIuIRDkFuYhIlFOQi4hEOQW5iEiUU5CLiEQ5BbmISJRTkIuIRDlzzkX+omb1wO6zfHs20BDCcqLFSGz3SGwzjMx2j8Q2w5m3e4xzLqf3Tk+CfDDMbIVzrtLrOiJtJLZ7JLYZRma7R2KbIXTt1tCKiEiUU5CLiES5aAzyB7wuwCMjsd0jsc0wMts9EtsMIWp31I2Ri4jIyaKxRy4iIkEU5CIiUW7IBrmZXWlmW8xsu5l9qY/jCWb2SOD4cjMri3yVoTWANn/OzDaa2Voze9HMxnhRZ6j11+6g864zM2dmUT9NbSBtNrMbA7/vDWb2cKRrDIcB/B0vNbOXzeydwN/z93lRZyiZ2c/NrM7M1p/iuJnZDwL/TNaa2awzvohzbsi9gBhgB1AOxANrgCm9zvk0cF/g55uAR7yuOwJtvgRICvz8qWhv80DbHTgvFXgVWAZUel13BH7X44F3gNGB7Vyv645Qux8APhX4eQqwy+u6Q9Dui4BZwPpTHH8f8GfAgHnA8jO9xlDtkc8FtjvndjrnOoDfAYt6nbMIeDDw8+PApWZmEawx1Ppts3PuZedce2BzGVAc4RrDYSC/a4D/AL4BhP7JtZE3kDZ/Evixc+4QgHOuLsI1hsNA2u2AtMDP6cD+CNYXFs65V4GDpzllEfAr57cMyDCzgjO5xlAN8iJgT9D23sC+Ps9xznUBh4GsiFQXHgNpc7Db8f9XPNr12+7A/2qWOMuDrrcAAAROSURBVOeWRLKwMBrI73oCMMHM3jCzZWZ2ZcSqC5+BtPte4CNmthf4E/D3kSnNU2f67/676OHLUcjMPgJUAgu8riXczMwHfAe4zeNSIi0W//DKxfj/z+tVM5vunGvytKrw+zvgl865b5vZ+cBDZjbNOdfjdWFD2VDtke8DSoK2iwP7+jzHzGLx/29YY0SqC4+BtBkzuwy4B7jGOXcsQrWFU3/tTgWmAa+Y2S78Y4hPRfkXngP5Xe8FnnLOdTrnqoCt+IM9mg2k3bcDjwI45/4KJOJfWGo4G9C/+6czVIP8bWC8mY01s3j8X2Y+1eucp4CPBX6+HnjJBb45iFL9ttnMzgXuxx/iw2HMFPppt3PusHMu2zlX5pwrw//dwDXOuRXelBsSA/n7/QT+3jhmlo1/qGVnJIsMg4G0uxq4FMDMJuMP8vqIVhl5TwEfDcxemQccds4dOKNP8Pob3dN80/s+/L2QHcA9gX3/jv9fYvD/gh8DtgNvAeVe1xyBNr8A1AKrA6+nvK45Eu3ude4rRPmslQH+rg3/kNJGYB1wk9c1R6jdU4A38M9oWQ0s9LrmELT5t8ABoBP//2ndDtwJ3Bn0u/5x4J/JurP5+61b9EVEotxQHVoREZEBUpCLiEQ5BbmISJRTkIuIRDkFuYhIlFOQy7BhZt1mtjqwWuAaM/t84M7Q48fPNbOfBW1faWZvmdnmwPseMbPSoOPfM7OLTnO9b5nZe4O2f2dm0X7TjkQhTT+UYcPMWp1zKYGfc4GHgTecc18N7HsM+Jpzbo2ZTQP+iH/+8qbA8WuAJufcq2aWBSxxzs07zfXGAP/rnFsY2F4AfMQ598kwNlPkXRTkMmwEB3lguxz/3YTZQAqwwjk3MXDsIfx3A//iFJ+1GCh0zt0bWA7gp4FDMcA055wFzlsJvN85VxPo/e8Axjv/Qm4iEaGhFRm2nHM78QdvLv5FxoIX9p8KrDrN298DrAx8zgrn3Ezn3EzgWeBbQeetCpyL8y/stB2YEao2iAyEglxGigJOsWaHmWUFxsi3mtkXTnW+mX0Y/wMCgp9sUwcUnmZbJOwU5DJsBYZWuvGH6xH86/MctwF/KOOcawz0th/APwRD7/MDY+r34l/zpDvocxID555qWyTsFOQyLJlZDnAf8CPn/yJoE1ARdMp/A/cEVtg7Lino5xPnm1kG/oWPPuqc692rn8DJQza9t0XCTg+WkOFklJmtBuKALuAh/CsI4pzbbGbpZpbqnGtxzq0zs88CvzKzNKAB/xKqXw181hLgDvxfci4CxgD/e/xpgs65mWYWhz/sVwCYWR5wxDlXE5nmivhp1oqMGGb2f4EW59xP+z3Zf/7rwAfcKZ7KY2YfBGY55/416PObnXM/6+t8kXDR0IqMJD8BzuSpSp8HSk9zPBb4dtB2E397ILhIxKhHLiIS5dQjFxGJcgpyEZEopyAXEYlyCnIRkSinIBcRiXL/HzcoHe8EXnQTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnx579Fk9jBr"
      },
      "source": [
        "위의 loss plot에서 볼 수 있듯이, Generator는 Discriminator를 속이는 것에 성공할 수록 $D_{\\theta}(G_{\\phi}(z)) \\approx 1$ 낮은 loss를 갖게 됩니다. \n",
        "\n",
        "하지만, 이미지 생성의 난이도를 생각하면, 학습 초반에 Discriminator에 비해 Generator가 못하는 일은 자명한 일입니다. 이 때, D(G(z))가 0에 가까운 지점 $D_{\\theta}(G_{\\phi}(z)) \\approx 0$ 에서의 함수의 기울기가 너무 작기 때문에 학습 초반에 Generator가 충분한 양의 학습 시그널을 받지 못하게 되는 문제점이 발생하게 됩니다.\n",
        "\n",
        "따라서, 위의 식과 직관적으로 유사한 의미를 가지는 다른 loss function을 정의해보도록 하겠습니다.\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{G_{\\phi}} = -E_{z}[\\log(D_{\\theta}(G_{\\phi}(z))]  \\tag{2}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLoGHehfhjuo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "f406b086-c615-4899-ea4b-c2a64ccdc28c"
      },
      "source": [
        "plt.title('-log(D(G(z))')\n",
        "x = np.arange(0, 1.0, 0.01)\n",
        "y = -np.log(x)\n",
        "plt.xlabel('D(G(z))')\n",
        "plt.plot(x,y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f970a5b34d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha3rIQ2i_NiN"
      },
      "source": [
        "위의 loss plot에서 볼 수 있듯이, Generator는 여전히 Discriminator를 속이는 것에 성공할 수록 $D_{\\theta}(G_{\\phi}(z)) \\approx 1$ 낮은 loss를 갖게 됩니다. \n",
        "\n",
        "하지만, 이전과는 달리 $D_{\\theta}(G_{\\phi}(z)) \\approx 0$에서의 gradient가 크기 때문에 학습 초반에 이미지를 생성하지 못할 때 오히려 충분한 양의 학습 시그널을 받을 수 있게 됩니다.\n",
        "\n",
        "따라서, 이번 과제에서는 Generator의 Loss로 두번째 식을 사용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oojglLhdKf6m"
      },
      "source": [
        "# Utility Functions\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def save_checkpoint(model, save_path, device):\n",
        "    if not os.path.exists(os.path.dirname(save_path)):\n",
        "        os.makedirs(os.path.dirname(save_path))\n",
        "    torch.save(model.cpu().state_dict(), save_path)\n",
        "    model.to(device)\n",
        "\n",
        "def load_checkpoint(model, checkpoint_path, device):\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(\"Invalid path!\")\n",
        "        return\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    model.to(device)\n",
        "\n",
        "class FolderDataset(data.Dataset):\n",
        "    def __init__(self, folder):\n",
        "        self.folder = folder\n",
        "        self.image_list = os.listdir(folder)\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(os.path.join(self.folder, self.image_list[index]))\n",
        "        return self.transform(image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "\n",
        "# Trainer\n",
        "class Trainer():\n",
        "    def __init__(self, \n",
        "                 trainloader, \n",
        "                 testloader, \n",
        "                 generator, \n",
        "                 discriminator, \n",
        "                 criterion,\n",
        "                 g_optimizer, \n",
        "                 d_optimizer, \n",
        "                 device):\n",
        "        \"\"\"\n",
        "        trainloader: train data's loader\n",
        "        testloader: test data's loader\n",
        "        generator: generator\n",
        "        discriminator: discriminator\n",
        "        criterion: loss function to evaluate the model (e.g., BCE Loss)\n",
        "        g_optimizer: optimizer for generator\n",
        "        d_optimizer: optimizer for discriminator\n",
        "        \"\"\"\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "        self.G = generator\n",
        "        self.D = discriminator\n",
        "        self.criterion = criterion\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.device = device\n",
        "\n",
        "        # Make directory to save the images & models for a specific checkpoint\n",
        "        os.makedirs(os.path.join('./results/', 'images'), exist_ok=True)\n",
        "        os.makedirs(os.path.join('./results/', 'checkpoints'), exist_ok=True)\n",
        "        os.makedirs(os.path.join('./results/', 'evaluation'), exist_ok=True)\n",
        "        \n",
        "    def train(self, epochs = 1):\n",
        "        self.G.to(self.device)\n",
        "        self.D.to(self.device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            for iter, (real_img, _) in enumerate(self.trainloader):\n",
        "                self.G.train()\n",
        "                self.D.train()\n",
        "                \n",
        "                batch_size = real_img.size(0)\n",
        "                real_label = torch.ones(batch_size).to(self.device)\n",
        "                fake_label = torch.zeros(batch_size).to(self.device)\n",
        "                # get real CIFAR-10 image\n",
        "                real_img = real_img.to(self.device)\n",
        "                # initialize latent_vector to feed into the Generator\n",
        "                z = torch.randn(real_img.size(0), 256).to(self.device)\n",
        "\n",
        "                ##########################################################################################\n",
        "                # Discriminator Loss 구현                                                                #\n",
        "                # Note : Discriminator Loss는 Generator network의 parameter에 영향을 주지 않아야 합니다. #\n",
        "                #        detach() function을 참고하세요.                                                 #\n",
        "                #        https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html              #\n",
        "                ##########################################################################################\n",
        "                D_loss: torch.Tensor = None\n",
        "                ### YOUR CODE HERE (~ 4 lines)\n",
        "                real_out = self.D(real_img)\n",
        "                fake_img = self.G(z)\n",
        "                fake_out = self.D(fake_img.detach())\n",
        "                D_loss = self.criterion(real_out, real_label) + self.criterion(fake_out, fake_label)\n",
        "                ### END YOUR CODE\n",
        "\n",
        "                # TEST CODE\n",
        "                # (TEST의 통과가 맞는 구현을 보장하지는 못합니다. 일반적으로는 loss가 1.38~1.45 사이의 값이 나와야 합니다.)\n",
        "                if epoch == 0 and iter == 0:\n",
        "                    assert D_loss.detach().allclose(torch.tensor(1.4000), atol=2e-1), \\\n",
        "                    f\"Discriminator Loss of the model does not match expected result.\"\n",
        "                    print(\"==Discriminator loss function test passed!==\")\n",
        "\n",
        "                self.D.zero_grad()\n",
        "                D_loss.backward()\n",
        "                self.d_optimizer.step()\n",
        "\n",
        "                #######################################################\n",
        "                # Generator Loss 구현                                 #\n",
        "                # Note : 위의 정의된 두번 째 식을 사용해서 구현하세요 #\n",
        "                #######################################################\n",
        "                G_loss: torch.Tensor = None\n",
        "                ### YOUR CODE HERE (~ 3 lines)\n",
        "                fake_img = self.G(z)\n",
        "                fake_out = self.D(fake_img)\n",
        "                G_loss = self.criterion(fake_out, real_label)\n",
        "                ### END YOUR CODE\n",
        "\n",
        "                # Test code\n",
        "                # (TEST의 통과가 맞는 구현을 보장하지는 못합니다. 일반적으로는 loss가 1.35~1.52 사이의 값이 나와야 합니다.)\n",
        "                if epoch == 0 and iter == 0:\n",
        "                    assert G_loss.detach().allclose(torch.tensor(1.5), atol=2e-1), \\\n",
        "                    f\"Generator Loss of the model does not match expected result.\"\n",
        "                    print(\"==Generator loss function test passed!==\")\n",
        "\n",
        "                self.G.zero_grad()\n",
        "                G_loss.backward()\n",
        "                self.g_optimizer.step()\n",
        "\n",
        "            # verbose\n",
        "            end_time = time.time() - start_time\n",
        "            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n",
        "            print('Time [%s], Epoch [%d/%d], lossD: %.4f, lossG: %.4f'\n",
        "                  % (end_time, epoch+1, epochs, D_loss.item(), G_loss.item()))\n",
        "\n",
        "            # Save Images\n",
        "            fake_img = fake_img.reshape(fake_img.size(0), 3, 32, 32)\n",
        "            torchvision.utils.save_image(denorm(fake_img), os.path.join('./results/', 'images', 'fake_image-{:03d}.png'.format(epoch+1)))\n",
        "            if epoch % 10 == 0:\n",
        "                self.test()\n",
        "\n",
        "        # Save Checkpoints\n",
        "        save_checkpoint(self.G, os.path.join('./results', 'checkpoints', 'G_final.pth'), self.device)\n",
        "        save_checkpoint(self.D, os.path.join('./results', 'checkpoints', 'D_final.pth'), self.device)\n",
        "\n",
        "    def test(self):\n",
        "        print('Start computing Inception Score')\n",
        "        self.G.eval()\n",
        "        with torch.no_grad():\n",
        "            for iter in tqdm.tqdm(range(5000)):\n",
        "                z = torch.randn(1, 256).to(self.device)\n",
        "                fake_img = self.G(z)\n",
        "                torchvision.utils.save_image(denorm(fake_img), os.path.join('./results/', 'evaluation', 'fake_image-{:03d}.png'.format(iter)))\n",
        "\n",
        "        # Compute the Inception score\n",
        "        dataset = FolderDataset(folder = os.path.join('./results/', 'evaluation'))\n",
        "        Inception = Inception_Score(dataset)\n",
        "        score = Inception.compute_score(splits=1)\n",
        "        print('Inception Score : ', score)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKACzMg9SzDF"
      },
      "source": [
        "### Train\n",
        "\n",
        "자, 이제 학습을 진행해 보겠습니다.\n",
        "학습이 진행됨에 따라 generator가 생성하는 image는 \\\\\n",
        "[파일]->[results]->[images]에서 각 epoch별로 확인해보실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBuw5xCdIglG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad43e0f9-c6e4-442b-9bbd-944a568b1405"
      },
      "source": [
        "lr = 2e-4\n",
        "\n",
        "trainloader, testloader = create_dataloader()\n",
        "G = Generator()\n",
        "D = Discriminator()\n",
        "criterion = nn.BCELoss()\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "device = torch.device('cuda')\n",
        "\n",
        "trainer = Trainer(trainloader=trainloader, \n",
        "                  testloader=testloader,\n",
        "                  generator=G,\n",
        "                  discriminator=D,\n",
        "                  criterion=criterion,\n",
        "                  g_optimizer=g_optimizer,\n",
        "                  d_optimizer=d_optimizer,\n",
        "                  device=device)\n",
        "\n",
        "trainer.train(epochs=50)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==Discriminator loss function test passed!==\n",
            "==Generator loss function test passed!==\n",
            "Time [0:00:47], Epoch [1/50], lossD: 0.2177, lossG: 4.4834\n",
            "Start computing Inception Score\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:08<00:00, 571.05it/s]\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "79it [01:01,  1.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception Score :  2.0722152604102377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:47], Epoch [2/50], lossD: 0.2580, lossG: 2.2199\n",
            "Time [0:03:35], Epoch [3/50], lossD: 0.4077, lossG: 2.5191\n",
            "Time [0:04:23], Epoch [4/50], lossD: 0.4489, lossG: 2.7645\n",
            "Time [0:05:11], Epoch [5/50], lossD: 0.5359, lossG: 5.5390\n",
            "Time [0:06:00], Epoch [6/50], lossD: 0.4076, lossG: 3.6671\n",
            "Time [0:06:48], Epoch [7/50], lossD: 0.2684, lossG: 4.1791\n",
            "Time [0:07:36], Epoch [8/50], lossD: 0.1034, lossG: 4.0335\n",
            "Time [0:08:24], Epoch [9/50], lossD: 0.0918, lossG: 3.4361\n",
            "Time [0:09:12], Epoch [10/50], lossD: 0.2844, lossG: 4.5256\n",
            "Time [0:09:59], Epoch [11/50], lossD: 0.3611, lossG: 4.4674\n",
            "Start computing Inception Score\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:09<00:00, 550.04it/s]\n",
            "79it [01:00,  1.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception Score :  3.7484425122708998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:58], Epoch [12/50], lossD: 0.0975, lossG: 4.1358\n",
            "Time [0:12:45], Epoch [13/50], lossD: 0.5864, lossG: 1.1035\n",
            "Time [0:13:32], Epoch [14/50], lossD: 0.8942, lossG: 2.6309\n",
            "Time [0:14:20], Epoch [15/50], lossD: 0.0431, lossG: 4.4464\n",
            "Time [0:15:07], Epoch [16/50], lossD: 0.1853, lossG: 4.6425\n",
            "Time [0:15:54], Epoch [17/50], lossD: 0.0772, lossG: 4.7077\n",
            "Time [0:16:41], Epoch [18/50], lossD: 0.1442, lossG: 3.6719\n",
            "Time [0:17:29], Epoch [19/50], lossD: 0.1623, lossG: 4.0677\n",
            "Time [0:18:16], Epoch [20/50], lossD: 0.1256, lossG: 5.1543\n",
            "Time [0:19:03], Epoch [21/50], lossD: 0.1338, lossG: 4.3656\n",
            "Start computing Inception Score\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:08<00:00, 564.61it/s]\n",
            "79it [01:00,  1.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception Score :  3.950156360103655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:21:01], Epoch [22/50], lossD: 0.2091, lossG: 3.8985\n",
            "Time [0:21:48], Epoch [23/50], lossD: 0.0860, lossG: 5.2119\n",
            "Time [0:22:35], Epoch [24/50], lossD: 0.2178, lossG: 4.0088\n",
            "Time [0:23:22], Epoch [25/50], lossD: 0.0957, lossG: 4.8595\n",
            "Time [0:24:10], Epoch [26/50], lossD: 0.1863, lossG: 3.6236\n",
            "Time [0:24:57], Epoch [27/50], lossD: 0.0279, lossG: 4.5394\n",
            "Time [0:25:45], Epoch [28/50], lossD: 0.1260, lossG: 4.4608\n",
            "Time [0:26:32], Epoch [29/50], lossD: 0.0249, lossG: 5.4409\n",
            "Time [0:27:19], Epoch [30/50], lossD: 0.1852, lossG: 3.5242\n",
            "Time [0:28:06], Epoch [31/50], lossD: 0.1917, lossG: 2.5745\n",
            "Start computing Inception Score\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:08<00:00, 564.74it/s]\n",
            "79it [01:00,  1.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception Score :  4.212877004134691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:30:04], Epoch [32/50], lossD: 0.0743, lossG: 5.2845\n",
            "Time [0:30:51], Epoch [33/50], lossD: 0.1738, lossG: 4.6336\n",
            "Time [0:31:38], Epoch [34/50], lossD: 0.6574, lossG: 2.5570\n",
            "Time [0:32:25], Epoch [35/50], lossD: 0.0857, lossG: 3.9621\n",
            "Time [0:33:13], Epoch [36/50], lossD: 0.0477, lossG: 4.4689\n",
            "Time [0:34:00], Epoch [37/50], lossD: 0.2681, lossG: 6.3673\n",
            "Time [0:34:47], Epoch [38/50], lossD: 0.0640, lossG: 5.4857\n",
            "Time [0:35:34], Epoch [39/50], lossD: 0.3815, lossG: 2.1046\n",
            "Time [0:36:21], Epoch [40/50], lossD: 0.1141, lossG: 3.5499\n",
            "Time [0:37:08], Epoch [41/50], lossD: 0.0660, lossG: 6.5942\n",
            "Start computing Inception Score\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:09<00:00, 542.02it/s]\n",
            "79it [01:00,  1.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception Score :  3.9950488695627064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:39:06], Epoch [42/50], lossD: 0.0250, lossG: 6.8565\n",
            "Time [0:39:54], Epoch [43/50], lossD: 0.1399, lossG: 5.1342\n",
            "Time [0:40:41], Epoch [44/50], lossD: 0.3743, lossG: 2.3584\n",
            "Time [0:41:28], Epoch [45/50], lossD: 0.1101, lossG: 6.0654\n",
            "Time [0:42:15], Epoch [46/50], lossD: 0.0939, lossG: 5.3856\n",
            "Time [0:43:03], Epoch [47/50], lossD: 0.0796, lossG: 4.6636\n",
            "Time [0:43:50], Epoch [48/50], lossD: 0.0948, lossG: 5.2483\n",
            "Time [0:44:37], Epoch [49/50], lossD: 0.0683, lossG: 5.4519\n",
            "Time [0:45:24], Epoch [50/50], lossD: 0.0130, lossG: 6.7613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y2faWkGQ5WG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Quiz2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhAEjARBRPGb"
      },
      "source": [
        "# NLP Quiz2\n",
        "\n",
        "> 본 퀴즈에서는 강의에서 직접적으로 다루지 않은 문제들도 포함되어 있습니다. 성취도 확인용으로 진행되는 시험이니 한 주 동안 배운 내용을 복습하고 새로운 문제들을 공부하는 데 의미를 두시면 좋을 것 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSqNrv0c52kO"
      },
      "source": [
        "### Part 1. 다음 명제에 대해, True/False를 판단하시오. 판단 근거를 간략하게 설명하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z081ktX4Ujo"
      },
      "source": [
        "Character 혹은 subword-level encoding은 word-level encoding이 갖고 있는 out-of-vocabulary problem을 해결한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RJeOeKRRfSs"
      },
      "source": [
        "A: True / oov를 해결하기위해 subword가 도입됨.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFl5wV-5daU9"
      },
      "source": [
        "RNN이나 LSTM은 character-level 의 표현형을 처리할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR1O9-Ued58u"
      },
      "source": [
        "A: True / 예시로 language model이 있음.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fNePCDvnrV7"
      },
      "source": [
        "L1, L2 distance와 같이, 두 벡터간의 Inner product의 값이 작으면,  거리가 가깝다고 볼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPnz4WWKnmtT"
      },
      "source": [
        "A: True / 거리가 가깝다는 것은 방향과 크기가 비슷하다는 것을 의미함.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1w82GsEq64J"
      },
      "source": [
        "RNN으로 구성된 Seq2Seq 에서는, decoder에서 self-attention을 활용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6ZSg1GjrGWR"
      },
      "source": [
        "A: \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T_L6Mg15h-K"
      },
      "source": [
        "Transformer에서는 Activation function으로 Tanh를 사용한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnLFmvNJ5h-K"
      },
      "source": [
        "A: False / ReLU를 사용함.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG6i1pj4gtKP"
      },
      "source": [
        "### Part 2. 객관식/주관식 문항"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6dnCH-yhhN0"
      },
      "source": [
        "해당 수식에 알맞는 Transformer Block의 구성요소는?\n",
        "\n",
        "\n",
        "$x$ is input of previous layer. $W$ and $b$ are weight and bias respectively.\n",
        "\n",
        "\n",
        "\n",
        "**Formula** : $max(0,xW_1+b_1)W_2+b_2$\n",
        "\n",
        "\n",
        "(객관식)\n",
        "1. Self-attention (v)\n",
        "2. Residual Connection \n",
        "3. Position-wise Feed Forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dL5bQ0_2CXv"
      },
      "source": [
        "Byte-pair encoding vocabulary  V = {[PAD], a, b, c, bc, bb, bcc} 가 주어졌을때 \n",
        "‘abbccc’를 indexing한 결과값은? (해당 vocabulary의 index는 0, 1, 2...  순서이며, left-first로 매칭된다) \\\n",
        "\n",
        "\n",
        "(객관식)\n",
        "\n",
        "A.    122333 \\\n",
        "B.     1263 \\\n",
        "C.     15333 (v) \\\n",
        "D.    없음 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2x6J8wW1rS7"
      },
      "source": [
        "\n",
        "Transformer에서 Attention weight를 계산하는데 있어서 $\\sqrt{d_k}$ 로 나눠주는 이유에 대해 주어진 expectation & variance property 와\n",
        "query key vector의 component q, k의 mean, variance를 이용해 설명하세요.  (주관식)\\\n",
        "$1. \\text{E}(aX)= a \\text{E}(X) $\\\n",
        "$2. \\text{Var}(X)= \\text{E}(X^2) - \\{E(X)\\}^2$ \\\n",
        "$3. \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y),$ \\\n",
        "$4. \\text{Var}(aX) = a^2\\text{Var}(X),$ \\\n",
        "$5. \\text{Var}(XY) = \\text{E}(X^2)\\text{E}(Y^2) - [\\text{E}(X)]^2[\\text{E}(Y)] ^2.$ \\\n",
        "\n",
        "Assumption : Query and Key vector are composed with $q$ and $k$ and its expectation and variance are 0 and 1 respectively.\n",
        "\n",
        "Formulate as : \\\n",
        "$ \\text{query vector }q = [q_0,q_1,...,q_n] $ \\\n",
        "$ \\text{key vector }k = [k_0,k_1,...,k_n] $ \\\n",
        "$ \\text{E}(q_i)=\\text{E}(k_i)=0, \\text{Var}(q_i)=\\text{Var}(k_i)=1 $  \n",
        "\n",
        " $ \\text{Attention}(Q, K) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpSBFen22Tcz"
      },
      "source": [
        "A:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4JvOeA70f8H"
      },
      "source": [
        "\\Sinusoidal Position Embedding의 수식을 써넣으세요  \\\n",
        "($pos$ is token position index, $2i$ and $2i+1$ are the ood and even index of dimension) (주관식)\n",
        "\n",
        "\n",
        "$ {PE}(pos, 2i) = $\n",
        "\n",
        "$ {PE}(pos, 2i+1) = $ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoSeTB3W0fwv"
      },
      "source": [
        "**도전문제.** 위 수식중 pos와 i를 기반으로, 각각 input의 위치에 다라 unique한 position embedding을 가질수 있는지 설명하세요. (주관식)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu1k1JLl2ddD"
      },
      "source": [
        "A:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzOY-7w66tPH"
      },
      "source": [
        "**도전문제.** 아래의 공식을 이용해, Sinusoidal Position embedding이 절대적인 Position 정보를 고려(두번째 네번째간의 거리는 다섯번째 일곱번째의 거리와 같음)할 수 있게 되는지 서술하세요 (주관식)\n",
        "\n",
        "$$sin(\\alpha + \\beta) = sin\\alpha * cos\\beta + cos\\alpha *sin\\beta $$\n",
        "\n",
        "$$cos(\\alpha + \\beta) = cos\\alpha * cos\\beta - sin\\alpha *sin\\beta $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhy57dc_6tPH"
      },
      "source": [
        "A:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziSqQz97Rjy"
      },
      "source": [
        "**도전문제.** Position의 특성을 고려한 새로운 Position Embedding을 제시해보세요 (합리적으로) (주관식)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVRuUk5C7Rjy"
      },
      "source": [
        "A:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCMbj_7ETutI"
      },
      "source": [
        "**도전문제.**  Transformer의 원 논문에서는 activation function으로 RELU를 사용합니다. BERT는 GeLU를 사용하는데, ReLU 대비 GeLU를 사용하면 좋은 점에대해 서술해보세요. (주관식)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hln3EkLn3_NS"
      },
      "source": [
        "A:\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HW24_Answer] NaiveBayes Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btDmoiRCRfMp"
      },
      "source": [
        "# **[HW24] NaiveBayes Classifier**\n",
        "1. Requirements\n",
        "2. Data Preprocessing\n",
        "3. Model Training\n",
        "4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a3E1pbkSYSF"
      },
      "source": [
        "## 1. Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4THGPr9QIjQY"
      },
      "source": [
        "#### 1.1 필요한 패키지를 설치(install) 및 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GKm6PwhiGxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7eb1e3-d08a-4a12-fe05-a82a71bade2d"
      },
      "source": [
        "# 한국어 전처리 라이브러리 \n",
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2srhF-lp4JxL"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# POS(Part of Speech) tagger\n",
        "from konlpy import tag "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ-s9c77IVUA"
      },
      "source": [
        "#### 1.2 Train data 와 test data 를 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCBnEQrfoL_C"
      },
      "source": [
        "data = {}\n",
        "# training data. input text 와 정답 label (긍정(1), 부정(0)) 로 구성.\n",
        "\n",
        "data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n",
        "                {'text': \"기대했던 것보단 별로였네요.\"},\n",
        "                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n",
        "                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n",
        "                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n",
        "                {'text': \"연기가 좀 별로였습니다.\"},\n",
        "                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n",
        "                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n",
        "                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n",
        "                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n",
        "                ]\n",
        "# test data\n",
        "data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n",
        "                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n",
        "                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n",
        "                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n",
        "                ]\n",
        "\n",
        "train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
        "test_labels = [1, 0, 1, 0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpgjbzqr6UR4"
      },
      "source": [
        "### 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKwnFBZqMXm-"
      },
      "source": [
        "#### 2.1 한글 형태소 분석기를 이용해서 주어진 데이터를 tokenize 합니다.\n",
        "\n",
        "오픈소스 형태소 분석기를 제공하는 파이썬 패키지 KoNLPy에서 제공하는 [꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용하여 tokenize 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEzeYDmPjNLl"
      },
      "source": [
        "# 형태소 분석기 선언\n",
        "morph_analyzer = tag.Kkma() "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tftxirx_7sk7"
      },
      "source": [
        "# tokenization 함수 정의\n",
        "def tokenization(data, morph_analyzer):\n",
        "    '''\n",
        "    (input) data: list of data examples.\n",
        "            morph_analyzer: morphological analyzer.\n",
        "    (output) tokenized_data: list of tokenized data examples.\n",
        "    '''\n",
        "    tokenized_data = []\n",
        "\n",
        "    for example in tqdm(data):\n",
        "        tokens = morph_analyzer.morphs(example['text'])\n",
        "        tokenized_data.append(tokens)\n",
        "\n",
        "    return tokenized_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4VEZyFlCqi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb7dc6b-f9f7-4bbb-aa75-2de3a4f730bb"
      },
      "source": [
        "# tokenization 함수를 이용한 데이터 tokenization\n",
        "tokenized_data = {}\n",
        "\n",
        "tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n",
        "tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n",
            "100%|██████████| 4/4 [00:00<00:00, 17.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEhn3uv2o2kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a15e561-9b0f-49d4-f461-985ccd8a70d6"
      },
      "source": [
        "# tokenized_data 확인\n",
        "tokenized_data['train']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n",
              " ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n",
              " ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n",
              " ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n",
              " ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n",
              " ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n",
              " ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n",
              " ['기념일',\n",
              "  '에',\n",
              "  '방문',\n",
              "  '하',\n",
              "  '었',\n",
              "  '는데',\n",
              "  '연기',\n",
              "  '도',\n",
              "  '연출',\n",
              "  '도',\n",
              "  '다',\n",
              "  '좋',\n",
              "  '았',\n",
              "  '습니다',\n",
              "  '.'],\n",
              " ['전반적',\n",
              "  '으로',\n",
              "  '지루',\n",
              "  '하',\n",
              "  '었',\n",
              "  '습니다',\n",
              "  '.',\n",
              "  '저',\n",
              "  '는',\n",
              "  '별',\n",
              "  '로',\n",
              "  '이',\n",
              "  '었',\n",
              "  '네요',\n",
              "  '.'],\n",
              " ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olCz4j6VS0KJ"
      },
      "source": [
        "#### 2.2 tokenization 결과를 이용해서 word to index dictionary 를 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2DXYDTTTChQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef51e60b-ba40-46fb-d1d3-e83b296de885"
      },
      "source": [
        "# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n",
        "tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i] ]\n",
        "unique_train_tokens = set(tokens)\n",
        "\n",
        "# NaiveBayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n",
        "word2index = defaultdict() # key: word, value: index of word\n",
        "idx = 0\n",
        "for token in tqdm(unique_train_tokens):\n",
        "    word2index[token] = idx\n",
        "    idx += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56/56 [00:00<00:00, 64017.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85oCOe0Xqcwk"
      },
      "source": [
        "### 3. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3uuFi52qjh6"
      },
      "source": [
        "#### 3.1 NaiveBayes Classifier 모델 클래스를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsZlgjkBHAod"
      },
      "source": [
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, word2index, k=0.1):\n",
        "        \"\"\"\n",
        "        (input) word2index: mapping a word to a pre-assigned index.\n",
        "        \"\"\"\n",
        "        self.k = k # for smoothing\n",
        "        self.word2index = word2index\n",
        "        self.priors = {} # Prior probability for each class, P(c)\n",
        "        self.likelihoods = {} # Likelihood for each token, P(d|c)\n",
        "\n",
        "    def _set_priors(self, labels):\n",
        "        \"\"\"\n",
        "        Set prior probability for each class, P(c).\n",
        "        Count the number of each class and calculate P(c) for each class.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Count the number of each class\n",
        "        class_counts = defaultdict(int)\n",
        "        for label in tqdm(labels):\n",
        "            class_counts[label] += 1\n",
        "        \n",
        "        # For each class, calcuate P(c)\n",
        "        for label, count in class_counts.items():\n",
        "            self.priors[label] = class_counts[label] / len(labels)\n",
        "\n",
        "    def _set_likelihoods(self, tokens, labels):\n",
        "        \"\"\"\n",
        "        Set likelihood for each token, P(d|c).\n",
        "        First, count the number of each class for each token.\n",
        "        Then, calculate P(d|c) for a given class and token.\n",
        "        \"\"\"\n",
        "        token_dists = {}\n",
        "        number_of_token_for_class = defaultdict(int)\n",
        "\n",
        "        for i, label in enumerate(tqdm(labels)):\n",
        "            count = 0\n",
        "\n",
        "            for token in tokens[i]:\n",
        "                if token in self.word2index:\n",
        "                    if token not in token_dists:\n",
        "                        token_dists[token] = {0:0, 1:0}\n",
        "                    token_dists[token][label] += 1\n",
        "                count += 1\n",
        "            number_of_token_for_class[label] += count\n",
        "        print(token_dists)\n",
        "\n",
        "        for token, dist in tqdm(token_dists.items()):\n",
        "            if token not in self.likelihoods:\n",
        "                self.likelihoods[token] = {\n",
        "                    0: (token_dists[token][0]+ self.k) / (number_of_token_for_class[0] + len(self.word2index)* self.k),\n",
        "                    1: (token_dists[token][1]+ self.k) / (number_of_token_for_class[1] + len(self.word2index)* self.k),\n",
        "                }\n",
        "\n",
        "    def train(self, input_tokens, labels):\n",
        "        \"\"\"\n",
        "        (input) input_tokens: list of tokenized train data.\n",
        "                labels: train labels for each sentence/document.\n",
        "        \"\"\"\n",
        "        self._set_priors(labels)\n",
        "        self._set_likelihoods(input_tokens, labels)\n",
        "\n",
        "    def inference(self, input_tokens):\n",
        "        \"\"\"\n",
        "        (input) input_tokens: list_of tokenized test data.\n",
        "        \"\"\"\n",
        "        log_prob_0 = 0.0\n",
        "        log_prob_1 = 0.0\n",
        "\n",
        "        for token in input_tokens:\n",
        "            if token in self.likelihoods:\n",
        "                log_prob_0 += math.log(self.likelihoods[token][0])\n",
        "                log_prob_1 += math.log(self.likelihoods[token][1])\n",
        "\n",
        "        log_prob_0 += math.log(self.priors[0])\n",
        "        log_prob_1 += math.log(self.priors[1])\n",
        "\n",
        "        if log_prob_0 >= log_prob_1:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzjVUyBOQEJk"
      },
      "source": [
        "#### 3.2 주어진 학습 데이터에 대해 문장 분류 모델을 학습시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt-iUEVRNsRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6ed58e-756a-403f-baff-4d4bcf29edbd"
      },
      "source": [
        "# 문장 분류 모델 선언 및 학습\n",
        "classifier = NaiveBayesClassifier(word2index)\n",
        "classifier.train(tokenized_data['train'], train_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 8760.03it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 32974.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'정말': {0: 0, 1: 1}, '재미있': {0: 0, 1: 1}, '습니다': {0: 3, 1: 4}, '.': {0: 6, 1: 6}, '추천': {0: 0, 1: 1}, '하': {0: 3, 1: 2}, 'ㅂ니다': {0: 0, 1: 3}, '기대': {0: 1, 1: 0}, '었': {0: 6, 1: 2}, '더': {0: 2, 1: 0}, 'ㄴ': {0: 2, 1: 0}, '것': {0: 1, 1: 0}, '보다': {0: 1, 1: 0}, '별': {0: 3, 1: 0}, '로': {0: 3, 1: 0}, '이': {0: 4, 1: 2}, '네요': {0: 3, 1: 0}, '지루': {0: 2, 1: 0}, '어서': {0: 1, 1: 0}, '다시': {0: 1, 1: 1}, '보': {0: 1, 1: 1}, '고': {0: 1, 1: 2}, '싶': {0: 1, 1: 1}, '다는': {0: 1, 1: 0}, '생각': {0: 1, 1: 0}, '안': {0: 1, 1: 0}, '들': {0: 1, 1: 1}, '완전': {0: 0, 1: 1}, '최고': {0: 0, 1: 2}, '!': {0: 0, 1: 1}, '연기': {0: 1, 1: 3}, '도': {0: 0, 1: 6}, '연출': {0: 0, 1: 3}, '다': {0: 0, 1: 2}, '만족': {0: 0, 1: 1}, '스럽': {0: 0, 1: 1}, '가': {0: 1, 1: 0}, '좀': {0: 1, 1: 0}, '좋': {0: 1, 1: 2}, '았': {0: 0, 1: 2}, '배우': {0: 0, 1: 1}, '분': {0: 0, 1: 1}, '기념일': {0: 0, 1: 1}, '에': {0: 1, 1: 1}, '방문': {0: 0, 1: 1}, '는데': {0: 0, 1: 1}, '전반적': {0: 1, 1: 0}, '으로': {0: 1, 1: 0}, '저': {0: 1, 1: 0}, '는': {0: 1, 1: 0}, 'CG': {0: 1, 1: 0}, '조금': {0: 1, 1: 0}, '신경': {0: 1, 1: 0}, '쓰': {0: 1, 1: 0}, '으면': {0: 1, 1: 0}, '겠': {0: 1, 1: 0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56/56 [00:00<00:00, 156170.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uryob_zz645R",
        "outputId": "d8e4f2ad-ced0-41c9-a1b2-b364b2de2f7a"
      },
      "source": [
        "classifier.likelihoods"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '.': {0: 0.08764367816091954, 1: 0.09591194968553458},\n",
              " 'CG': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " 'ㄴ': {0: 0.030172413793103453, 1: 0.0015723270440251573},\n",
              " 'ㅂ니다': {0: 0.0014367816091954025, 1: 0.04874213836477988},\n",
              " '가': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '것': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '겠': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '고': {0: 0.015804597701149427, 1: 0.0330188679245283},\n",
              " '기념일': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '기대': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '네요': {0: 0.04454022988505748, 1: 0.0015723270440251573},\n",
              " '는': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '는데': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '다': {0: 0.0014367816091954025, 1: 0.0330188679245283},\n",
              " '다는': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '다시': {0: 0.015804597701149427, 1: 0.01729559748427673},\n",
              " '더': {0: 0.030172413793103453, 1: 0.0015723270440251573},\n",
              " '도': {0: 0.0014367816091954025, 1: 0.09591194968553458},\n",
              " '들': {0: 0.015804597701149427, 1: 0.01729559748427673},\n",
              " '로': {0: 0.04454022988505748, 1: 0.0015723270440251573},\n",
              " '만족': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '방문': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '배우': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '별': {0: 0.04454022988505748, 1: 0.0015723270440251573},\n",
              " '보': {0: 0.015804597701149427, 1: 0.01729559748427673},\n",
              " '보다': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '분': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '생각': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '스럽': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '습니다': {0: 0.04454022988505748, 1: 0.06446540880503145},\n",
              " '신경': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '싶': {0: 0.015804597701149427, 1: 0.01729559748427673},\n",
              " '쓰': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '안': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '았': {0: 0.0014367816091954025, 1: 0.0330188679245283},\n",
              " '어서': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '었': {0: 0.08764367816091954, 1: 0.0330188679245283},\n",
              " '에': {0: 0.015804597701149427, 1: 0.01729559748427673},\n",
              " '연기': {0: 0.015804597701149427, 1: 0.04874213836477988},\n",
              " '연출': {0: 0.0014367816091954025, 1: 0.04874213836477988},\n",
              " '완전': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '으로': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '으면': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '이': {0: 0.05890804597701149, 1: 0.0330188679245283},\n",
              " '재미있': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '저': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '전반적': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '정말': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '조금': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '좀': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n",
              " '좋': {0: 0.015804597701149427, 1: 0.0330188679245283},\n",
              " '지루': {0: 0.030172413793103453, 1: 0.0015723270440251573},\n",
              " '최고': {0: 0.0014367816091954025, 1: 0.0330188679245283},\n",
              " '추천': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n",
              " '하': {0: 0.04454022988505748, 1: 0.0330188679245283}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h79XWrsnQJtN"
      },
      "source": [
        "### 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjk05W136d5o"
      },
      "source": [
        "각각의 Test 데이터에 대해 정답값을 추론하고 Accuracy를 구합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe-fOScGNzH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f1bb1b-f3ca-4818-d9e4-059094b9e50f"
      },
      "source": [
        "# Test 데이터 inference\n",
        "preds = []\n",
        "for test_tokens in tqdm(tokenized_data['test']):\n",
        "    pred = classifier.inference(test_tokens)\n",
        "    preds.append(pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 19485.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrYMTKM10vYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842d0a79-b6a0-4318-9e7c-7a14e21bd1a0"
      },
      "source": [
        "# Accuracy 측정\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(test_labels, preds))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b68-Bh5zArE"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}
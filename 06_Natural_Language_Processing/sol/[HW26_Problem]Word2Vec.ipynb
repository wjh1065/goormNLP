{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[HW26_Problem]Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "##**3. Word2Vec**\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\n",
        "2. CBOW, Skip-gram 모델을 각각 구현합니다.\n",
        "3. 모델을 실제로 학습해보고 결과를 확인합니다.\n",
        "4. 산점도를 그려 단어들의 대략적인 위치를 확인해봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utBdiiW499DI",
        "outputId": "0a5f5d71-20cd-41dd-e756-c79a899b4143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 0s (21.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "outputId": "3073bdf9-9a6f-4e3c-f988-a1d9eace8244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from konlpy.tag import Mecab,Twitter,Okt,Kkma\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "\n",
        "\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\n",
        "  \"정말 맛있습니다. 추천합니다.\",\n",
        "  \"기대했던 것보단 별로였네요.\",\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
        "]\n",
        "\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "tokenizer = Okt()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\n",
        "  tokenized = []\n",
        "  for sent in tqdm(data):\n",
        "    tokens = tokenizer.morphs(sent, stem=True)\n",
        "    tokenized.append(tokens)\n",
        "\n",
        "  return tokenized"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "outputId": "183c800b-8c5a-4552-8a33-89cda2e43d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)\n",
        "train_tokenized"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 60.04it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['정말', '맛있다', '.', '추천', '하다', '.'],\n",
              " ['기대하다', '것', '보단', '별로', '이다', '.'],\n",
              " ['다',\n",
              "  '좋다',\n",
              "  '가격',\n",
              "  '이',\n",
              "  '너무',\n",
              "  '비싸다',\n",
              "  '다시',\n",
              "  '가다',\n",
              "  '싶다',\n",
              "  '생각',\n",
              "  '이',\n",
              "  '안',\n",
              "  '드네',\n",
              "  '요',\n",
              "  '.'],\n",
              " ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'],\n",
              " ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'],\n",
              " ['위생',\n",
              "  '상태',\n",
              "  '가',\n",
              "  '좀',\n",
              "  '별로',\n",
              "  '이다',\n",
              "  '.',\n",
              "  '좀',\n",
              "  '더',\n",
              "  '개선',\n",
              "  '되다',\n",
              "  '기르다',\n",
              "  '바라다',\n",
              "  '.'],\n",
              " ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'],\n",
              " ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'],\n",
              " ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'],\n",
              " ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "outputId": "95a4ec54-4cbc-41a6-90e6-01cc405e2a02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count = defaultdict(int)\n",
        "\n",
        "for tokens in tqdm(train_tokenized):\n",
        "  for token in tokens:\n",
        "    word_count[token] += 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 5088.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "outputId": "5968046c-f29b-480d-d36a-0b9d27faebfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "print(list(word_count))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "outputId": "046ff20b-896b-4f55-cbb4-5548ad1ffdac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w2i = {}\n",
        "for pair in tqdm(word_count):\n",
        "  if pair[0] not in w2i:\n",
        "    w2i[pair[0]] = len(w2i)\n",
        "\n",
        "i2w={v:k for k,v in w2i.items()}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 272652.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiGqiEGDL5B_",
        "outputId": "5442b4e8-3c86-4a3a-d1b6-57ee8c39fa3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_tokenized)\n",
        "print()\n",
        "print(w2i)\n",
        "print()\n",
        "print(i2w)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
            "\n",
            "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n",
            "\n",
            "{0: '.', 1: '도', 2: '이다', 3: '좋다', 4: '별로', 5: '다', 6: '이', 7: '너무', 8: '음식', 9: '서비스', 10: '하다', 11: '방문', 12: '위생', 13: '좀', 14: '더', 15: '에', 16: '조금', 17: '정말', 18: '맛있다', 19: '추천', 20: '기대하다', 21: '것', 22: '보단', 23: '가격', 24: '비싸다', 25: '다시', 26: '가다', 27: '싶다', 28: '생각', 29: '안', 30: '드네', 31: '요', 32: '완전', 33: '최고', 34: '!', 35: '재', 36: '의사', 37: '있다', 38: '만족스럽다', 39: '상태', 40: '가', 41: '개선', 42: '되다', 43: '기르다', 44: '바라다', 45: '맛', 46: '직원', 47: '분들', 48: '친절하다', 49: '기념일', 50: '분위기', 51: '전반', 52: '적', 53: '으로', 54: '짜다', 55: '저', 56: '는', 57: '신경', 58: '써다', 59: '불쾌하다'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcm_L4iJBufO"
      },
      "source": [
        "### 다음은 Word2Vec을 학습시키는 대표적인 방법인 Skipgram과 CBoW를 다룹니다. \n",
        "\n",
        "* CboW는 주변단어를 이용해, 주어진 단어를 예측하는 방법입니다.\n",
        "* Skipgram은 중심 단어를 이용하여 주변 단어를 예측하는 방법입니다.\n",
        "* 즉 데이터셋을 구성할때, input x 와 target y를 어떻게 설정하는지에 차이가 있습니다.\n",
        "\n",
        "참고자료 \n",
        "\n",
        "* https://simonezz.tistory.com/35 \n",
        "\n",
        "* https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "class CBOWDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = [] # input word\n",
        "    self.y = [] # target word\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "          ############################ ANSWER HERE ################################\n",
        "          # TODO 1: insert tokens for input self.x\n",
        "          # TODO 2: insert tokens for targets self.y\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.y.append(id)\n",
        "          #########################################################################        \n",
        "\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "            ############################ ANSWER HERE ################################\n",
        "          # TODO 1: insert tokens for input self.x\n",
        "          # TODO 2: insert tokens for targets self.y\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.x += [id] * 2 * window_size\n",
        "          #########################################################################        \n",
        "\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAGV5IUUba0"
      },
      "source": [
        "각 모델에 맞는 `Dataset` 객체를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "outputId": "8794f00c-2a16-4da6-dea8-b875b4ec2438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 19812.49it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 35394.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### **모델 Class 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \n",
        "\n",
        "\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x):  # x: (B, 2W)\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x): # x: (B)\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cJalkDWYMT"
      },
      "source": [
        "두 가지 모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWUXEi8WeM-"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### **모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "outputId": "6d37881c-b3de-4414-84e3-cf7bb81578db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cbow.train()\n",
        "cbow = cbow.to(device)\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(cbow_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = cbow(x)  # (B, V)\n",
        " \n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 449.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.297618389129639\n",
            "Train loss: 4.217676162719727\n",
            "Train loss: 3.585211992263794\n",
            "Train loss: 3.36354923248291\n",
            "Train loss: 2.841198682785034\n",
            "Train loss: 2.93984317779541\n",
            "Train loss: 3.7310338020324707\n",
            "Train loss: 3.9674315452575684\n",
            "Train loss: 3.597597599029541\n",
            "Train loss: 3.700366497039795\n",
            "Train loss: 4.280869960784912\n",
            "Train loss: 3.4715685844421387\n",
            "Train loss: 2.9101059436798096\n",
            "Train loss: 3.607541799545288\n",
            "Train loss: 3.99641752243042\n",
            "Train loss: 3.7235562801361084\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 368.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.122655868530273\n",
            "Train loss: 4.082393646240234\n",
            "Train loss: 3.4514763355255127\n",
            "Train loss: 3.271561622619629\n",
            "Train loss: 2.7428338527679443\n",
            "Train loss: 2.741809844970703\n",
            "Train loss: 3.5823068618774414\n",
            "Train loss: 3.8333847522735596\n",
            "Train loss: 3.496826410293579\n",
            "Train loss: 3.550662040710449\n",
            "Train loss: 4.127963066101074\n",
            "Train loss: 3.170661449432373\n",
            "Train loss: 2.8073055744171143\n",
            "Train loss: 3.5133094787597656\n",
            "Train loss: 3.8413846492767334\n",
            "Train loss: 3.611443519592285\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 443.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.9503185749053955\n",
            "Train loss: 3.9498980045318604\n",
            "Train loss: 3.3216278553009033\n",
            "Train loss: 3.181483745574951\n",
            "Train loss: 2.6468124389648438\n",
            "Train loss: 2.5568904876708984\n",
            "Train loss: 3.436779737472534\n",
            "Train loss: 3.702324390411377\n",
            "Train loss: 3.3992133140563965\n",
            "Train loss: 3.407315969467163\n",
            "Train loss: 3.9808244705200195\n",
            "Train loss: 2.8870298862457275\n",
            "Train loss: 2.707211494445801\n",
            "Train loss: 3.421675443649292\n",
            "Train loss: 3.689615488052368\n",
            "Train loss: 3.502812147140503\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 426.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.7807657718658447\n",
            "Train loss: 3.8201088905334473\n",
            "Train loss: 3.1955270767211914\n",
            "Train loss: 3.09324049949646\n",
            "Train loss: 2.553170919418335\n",
            "Train loss: 2.385996103286743\n",
            "Train loss: 3.2946722507476807\n",
            "Train loss: 3.574321746826172\n",
            "Train loss: 3.304776430130005\n",
            "Train loss: 3.2706336975097656\n",
            "Train loss: 3.8399503231048584\n",
            "Train loss: 2.6228291988372803\n",
            "Train loss: 2.609853506088257\n",
            "Train loss: 3.3325419425964355\n",
            "Train loss: 3.5412871837615967\n",
            "Train loss: 3.3975489139556885\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.6142020225524902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 16/16 [00:00<00:00, 427.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.6929829120635986\n",
            "Train loss: 3.0730535984039307\n",
            "Train loss: 3.0067596435546875\n",
            "Train loss: 2.4619407653808594\n",
            "Train loss: 2.2296345233917236\n",
            "Train loss: 3.1562235355377197\n",
            "Train loss: 3.4494504928588867\n",
            "Train loss: 3.2135229110717773\n",
            "Train loss: 3.14076566696167\n",
            "Train loss: 3.7057948112487793\n",
            "Train loss: 2.38008975982666\n",
            "Train loss: 2.5152535438537598\n",
            "Train loss: 3.245830535888672\n",
            "Train loss: 3.396615982055664\n",
            "Train loss: 3.2955527305603027\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "outputId": "a2fa36c0-a2ac-4aaa-8aac-a08366dba333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "skipgram.train()\n",
        "skipgram = skipgram.to(device)\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(skipgram_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = skipgram(x)  # (B, V)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 918.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.898638725280762\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 823.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.85079288482666\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 811.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.803236484527588\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 856.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.755971908569336\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 855.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.70900297164917\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### **테스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "outputId": "e27cc01a-cf5e-4a78-c42f-65ca799413d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = cbow.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(f\"w2i[word]: {w2i[word]}\")\n",
        "  print(emb.squeeze(0))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "w2i[word]: 8\n",
            "tensor([ 5.2992e-01,  2.1189e+00, -1.7974e+00, -3.5114e-01,  1.9243e+00,\n",
            "        -1.3335e+00,  4.9476e-02, -8.5000e-01, -1.5052e+00, -5.3449e-01,\n",
            "        -1.1746e+00, -7.4601e-01,  1.2954e+00,  1.6602e+00,  1.5289e-01,\n",
            "         2.7537e-01, -1.2049e+00,  1.0933e+00,  1.1584e-01, -2.9945e-01,\n",
            "        -6.8950e-01, -1.9850e-01,  3.4524e-01,  1.4285e+00, -1.7953e-01,\n",
            "        -1.0285e+00, -5.3260e-01, -6.9546e-01, -3.4045e-01, -1.4651e+00,\n",
            "        -9.8982e-01, -9.9681e-01,  2.9218e-01,  5.6220e-01,  4.7143e-01,\n",
            "        -1.6193e+00,  3.8939e-01,  1.7083e+00,  9.4096e-02, -1.5825e-01,\n",
            "        -3.1324e-01, -6.9984e-01,  5.2397e-01, -8.8000e-01,  1.6525e+00,\n",
            "        -1.2585e+00,  2.0743e-02,  1.1960e-02, -1.1229e+00, -1.2404e+00,\n",
            "        -9.1786e-01, -1.1853e+00,  2.4117e-01,  5.3276e-01, -8.3940e-01,\n",
            "         2.1868e+00, -1.4669e+00, -4.0026e-01, -1.0568e+00, -8.1377e-01,\n",
            "         1.5899e+00,  9.4744e-01, -1.6827e+00,  1.1312e+00,  1.5501e+00,\n",
            "         6.6222e-01,  3.4621e-01,  1.3285e-01, -4.7225e-01, -7.3687e-01,\n",
            "        -1.6662e+00, -6.7341e-01,  3.6605e-01, -4.3753e-01,  7.0255e-02,\n",
            "         9.5853e-01,  3.0470e-01,  1.1827e-01, -3.9432e-02, -5.7262e-01,\n",
            "         1.0452e+00, -1.1744e+00,  4.0990e-01, -2.5375e-01,  9.4094e-01,\n",
            "        -1.4335e+00,  8.2376e-01,  6.0731e-01,  1.9612e+00,  8.7259e-01,\n",
            "         8.6826e-01, -9.1465e-01,  4.3517e-01,  1.4097e+00,  7.8913e-01,\n",
            "         3.1499e-01,  2.7088e-01, -8.8529e-02, -7.9390e-01, -1.0822e+00,\n",
            "        -1.4329e+00,  1.8062e-01,  7.3195e-01,  2.8405e-01,  1.2007e+00,\n",
            "         2.2814e-01,  4.2344e-02,  6.1921e-01, -1.8862e+00, -5.6755e-01,\n",
            "        -3.7591e-01,  7.7169e-02, -7.5450e-01,  8.6356e-01, -2.4295e+00,\n",
            "        -4.5416e-01, -8.1967e-01, -1.2124e+00, -7.4651e-01, -2.1643e-01,\n",
            "        -1.5463e+00, -1.2571e-01,  1.0566e+00,  3.3302e-01,  6.7754e-01,\n",
            "         1.3421e-01,  6.6412e-01, -4.9535e-01, -3.7249e-01,  7.1525e-01,\n",
            "        -9.1600e-01,  7.8179e-01,  1.3041e+00, -6.1943e-01,  9.0728e-01,\n",
            "        -5.6215e-01, -9.5260e-01,  1.8398e-01,  3.3488e-01,  1.3823e+00,\n",
            "        -9.6197e-01,  7.0508e-01,  6.4081e-01,  1.2629e+00,  7.9766e-01,\n",
            "         6.4922e-01,  9.7978e-01, -3.1478e-01,  7.1790e-01, -2.0669e+00,\n",
            "         1.8946e+00, -1.2151e+00, -3.3881e-01, -1.5045e+00,  1.7974e+00,\n",
            "         7.8892e-01, -5.3769e-01, -1.0861e-01,  9.6907e-01, -7.6210e-01,\n",
            "        -6.0144e-01, -3.6097e-01,  1.8025e+00, -7.3210e-01, -1.4264e+00,\n",
            "        -1.6351e-01,  4.2357e-01,  1.9702e+00, -6.7249e-01,  1.6443e+00,\n",
            "         1.1465e+00,  8.2030e-01, -1.6920e-02,  1.1372e+00,  1.0594e+00,\n",
            "        -5.3404e-01,  1.0245e+00,  1.5298e-01, -6.7131e-01, -3.1753e-01,\n",
            "        -1.3158e-01, -2.5358e-01,  7.2156e-01,  4.7335e-01, -1.0438e+00,\n",
            "        -6.5391e-01,  5.9001e-01, -1.7185e+00,  9.0999e-01, -4.8270e-01,\n",
            "        -3.3024e-01, -1.4670e-01,  9.7548e-03,  6.2117e-01,  1.3031e+00,\n",
            "         3.0478e-01, -2.3381e-01, -5.2440e-02, -1.2262e+00,  2.6432e+00,\n",
            "        -9.9034e-01,  4.6048e-01, -1.0555e-03, -9.7013e-01, -8.8055e-01,\n",
            "        -3.4407e-01,  1.0344e-02, -2.6117e-01,  1.0982e+00, -6.1826e-01,\n",
            "        -1.0996e+00, -2.1012e-01, -5.2513e-01,  6.1681e-01,  1.6616e+00,\n",
            "        -9.2833e-01, -1.1847e+00,  7.3632e-04, -6.4064e-02,  5.7209e-01,\n",
            "         1.5313e+00,  1.2414e+00, -1.1345e+00, -1.5078e+00, -5.4391e-01,\n",
            "         1.1585e+00,  6.5111e-01,  9.4057e-01,  1.2749e-01, -1.4731e+00,\n",
            "        -1.6076e+00,  9.8721e-01, -1.3829e+00,  3.4228e-01, -1.1947e+00,\n",
            "         1.0355e+00, -8.5345e-02,  1.8322e-01, -5.7474e-01, -3.2950e-01,\n",
            "         3.7362e-01,  1.5430e+00, -1.0740e+00, -2.8852e+00, -1.3190e+00,\n",
            "        -4.0541e-01,  1.7432e+00, -1.0130e-01, -1.7168e+00,  1.6963e+00,\n",
            "         1.2230e+00, -1.0826e+00,  2.8828e+00, -5.4835e-01,  1.1572e+00,\n",
            "        -4.7470e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 맛\n",
            "w2i[word]: 45\n",
            "tensor([-9.1761e-01,  6.4720e-01, -1.3517e+00,  3.9572e-01, -9.0211e-01,\n",
            "        -1.1083e+00,  8.9792e-01,  2.0054e-01, -4.2720e-01, -1.0035e-02,\n",
            "         2.5826e-01,  7.5293e-01,  2.0750e-01, -5.7952e-01,  8.0237e-01,\n",
            "         4.9276e-01, -9.6322e-01,  5.8869e-01, -5.8038e-01,  2.7867e-02,\n",
            "        -1.9645e-01,  6.6591e-01,  1.1877e-01, -7.7497e-01,  4.6356e-01,\n",
            "        -1.8283e+00, -4.1908e-01,  5.1264e-01, -8.1785e-01, -3.2428e-03,\n",
            "        -2.7469e-01,  5.3706e-02,  6.4142e-01, -3.0576e-01,  1.8838e-01,\n",
            "        -1.8471e+00,  6.1583e-01, -3.5087e-01,  4.2818e-01,  5.6743e-01,\n",
            "        -8.3710e-01, -1.5193e+00, -3.9334e-01, -2.6197e-01, -7.7766e-01,\n",
            "        -1.3481e+00, -4.7668e-02,  8.0054e-03,  9.0590e-01,  6.6693e-01,\n",
            "        -2.9184e-01,  4.1273e-01, -1.7255e-01, -1.6240e+00, -4.2450e-01,\n",
            "         3.3998e-01, -1.3857e+00,  5.5406e-01,  1.2864e+00,  2.0600e+00,\n",
            "        -4.5129e-02, -9.6410e-01, -4.9023e-01, -2.1044e+00, -4.7848e-01,\n",
            "         5.8689e-01,  9.5908e-03, -2.1510e-01, -2.5062e-01, -1.0008e+00,\n",
            "         2.0397e+00,  1.8073e+00,  2.2245e-01, -7.3243e-01, -1.0582e+00,\n",
            "         1.1195e+00,  4.9871e-01,  9.0882e-01, -8.5513e-01, -4.5948e-01,\n",
            "        -7.1748e-01,  1.3113e+00,  7.6186e-01,  1.2022e+00,  9.9312e-02,\n",
            "        -6.9832e-01,  4.6813e-01,  8.6766e-02, -5.2357e-01, -1.2886e+00,\n",
            "         3.5994e-02, -1.3942e+00, -1.9306e-01,  4.1088e-01, -6.5737e-01,\n",
            "         1.7135e+00,  1.6223e+00, -1.7279e+00, -9.9248e-01,  2.6339e+00,\n",
            "        -6.3332e-01,  6.2335e-01, -5.4244e-01, -4.2476e-01,  1.9108e+00,\n",
            "        -1.9894e+00, -2.8939e-01,  1.5054e+00, -2.0993e+00,  4.4062e-01,\n",
            "         1.4285e+00,  3.2926e-01, -9.7959e-01,  3.6984e-01, -5.1105e-01,\n",
            "        -1.0860e+00, -8.1130e-01, -2.4630e-01,  7.3807e-01, -1.8924e+00,\n",
            "        -1.2806e+00, -7.7540e-01, -2.0435e+00, -5.1651e-01,  9.7659e-01,\n",
            "        -6.1728e-01, -9.5992e-01, -1.3041e+00, -3.8139e-01,  6.5539e-01,\n",
            "         8.3867e-01, -1.2928e+00,  1.4839e+00, -8.8188e-01, -1.1199e+00,\n",
            "        -3.7914e-01,  2.2339e+00, -8.0617e-01,  4.8065e-01,  2.1857e-01,\n",
            "        -2.1868e-01,  6.4305e-01,  1.9429e-01,  1.6685e+00,  1.0223e+00,\n",
            "        -4.4602e-01,  8.0787e-01,  8.9693e-01, -9.1617e-01,  1.0994e+00,\n",
            "        -6.5460e-01, -1.3045e+00, -1.5122e+00,  1.2292e+00, -1.7967e+00,\n",
            "         5.1965e-01, -1.1106e-01, -2.5711e-03, -2.5583e-01, -5.6339e-01,\n",
            "         1.2128e+00,  5.1427e-01,  1.0075e+00, -3.1619e+00, -2.8174e-01,\n",
            "         2.2008e+00,  4.3241e-01,  7.0395e-01, -2.5027e-01, -1.1761e+00,\n",
            "         5.8518e-01,  1.9572e-01, -3.6024e-01,  1.2287e-01,  5.6344e-01,\n",
            "        -1.4912e+00,  1.8473e-01,  6.6098e-01,  7.8249e-01,  1.3341e+00,\n",
            "         1.8805e+00, -7.3354e-01,  2.9515e-01,  8.0531e-01, -6.0532e-01,\n",
            "         3.4611e-01,  1.1824e+00, -2.7296e-01,  3.8553e+00,  1.8732e+00,\n",
            "        -1.3504e+00,  6.1689e-01,  1.7241e-01,  2.2185e-01,  8.7587e-02,\n",
            "        -8.1931e-01, -2.7432e-01, -4.5693e-01, -1.0992e+00,  2.7160e-01,\n",
            "         2.9826e-01,  1.8796e+00, -1.3270e+00,  9.7006e-01, -8.3559e-01,\n",
            "         6.6334e-01, -3.6672e-01,  4.8462e-01,  5.1875e-01, -9.5587e-01,\n",
            "        -1.5361e-02,  9.7914e-01, -3.6324e-01,  1.4735e+00, -1.9796e-01,\n",
            "         1.9361e+00,  7.0102e-01,  2.2305e+00,  4.8842e-01,  1.0353e+00,\n",
            "        -1.5570e+00, -5.4190e-01, -1.7389e-02,  1.0056e+00,  7.5956e-01,\n",
            "        -1.4908e-01, -1.8128e+00, -1.9003e+00, -6.5054e-02, -1.5846e-01,\n",
            "        -5.7377e-01,  5.8929e-01,  1.1208e+00, -9.1007e-01,  1.3109e-01,\n",
            "        -5.5737e-01, -1.5677e+00,  4.7520e-01,  1.2797e+00, -3.2409e-03,\n",
            "         4.6275e-01,  9.9172e-01,  4.0817e-01,  3.7921e-01,  5.1580e-02,\n",
            "         1.8912e+00,  6.9752e-01, -2.0657e+00,  1.4682e-02,  4.1274e-01,\n",
            "        -9.0967e-01,  9.8719e-01, -1.7345e+00, -1.6241e-01, -8.7415e-01,\n",
            "        -1.0907e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 서비스\n",
            "w2i[word]: 9\n",
            "tensor([ 1.2359,  1.1336, -0.1025, -0.3846,  1.2209,  0.0913, -0.2194,  0.1714,\n",
            "         0.1683, -0.2316, -1.0682,  0.0384, -0.2745, -1.3142, -0.1943,  0.3324,\n",
            "        -0.1613, -0.0614, -0.6871, -1.1501, -1.0941, -0.4193,  0.6902,  1.1034,\n",
            "        -0.4204, -2.1398,  1.0828,  2.3361, -0.1871,  0.2989,  0.6925, -0.1394,\n",
            "         1.7936, -0.4549,  0.2020, -0.5590, -1.3269,  1.2979, -0.2559,  0.0936,\n",
            "         0.8333, -0.7721, -0.3498,  0.2993,  0.1966, -0.0929, -0.1935, -0.1598,\n",
            "        -1.2371, -0.1130, -2.1616,  0.1381,  0.8022,  1.1804, -0.4354, -1.1863,\n",
            "        -0.6060,  0.2288, -0.4371,  1.0973, -0.7156,  0.4736,  1.4800,  0.0458,\n",
            "        -0.3341, -0.3958,  0.1006,  0.8667,  0.8472,  2.2953, -0.6054, -1.4518,\n",
            "         0.7950, -1.5398, -0.2753,  0.5452, -0.7775, -1.9201,  0.2613, -0.3893,\n",
            "        -0.3237, -1.8274, -1.3013, -1.7697,  0.6991, -2.1357,  1.1882, -1.2985,\n",
            "         1.2049, -0.9483,  2.7168,  0.3020, -1.6184,  0.8784,  1.3240,  0.1745,\n",
            "        -2.2866,  0.8391, -0.7076, -0.4539, -2.0515,  0.4203, -0.2296, -0.2316,\n",
            "         1.1830,  1.8716,  0.0695,  0.5315,  0.2513,  0.9417,  0.3569, -0.5104,\n",
            "         0.3736,  0.0279,  2.0723,  0.5762, -0.4457,  1.0678,  0.4400,  1.0445,\n",
            "        -0.6081, -1.0910, -0.0718, -0.0868,  1.0812, -0.4813,  0.8564, -0.7562,\n",
            "         0.6071,  0.3141,  0.3809,  1.2779,  0.8096,  0.6119,  0.0432, -2.0803,\n",
            "         0.4407, -0.1271, -0.2214, -1.2650,  0.1049, -0.2090, -0.3417, -0.5268,\n",
            "         1.2024,  1.0542, -1.2299,  0.2907, -1.5463,  1.0286, -0.9428,  0.8354,\n",
            "        -1.4222, -1.0133,  0.2813,  0.8530,  1.6939,  1.4203, -0.6483,  1.5630,\n",
            "         1.7100,  1.2560,  2.1567, -0.5189,  0.5133,  1.2814,  0.8803,  0.2637,\n",
            "        -0.1699, -0.1703, -0.8587, -0.2109,  0.3080,  0.0611, -0.5152,  1.2170,\n",
            "         0.0771,  0.1159, -0.0601,  0.4212,  0.6210, -0.0236,  2.9715,  0.2965,\n",
            "        -0.4655,  0.5059,  1.0561, -0.8937, -1.0443, -1.3593,  0.9818,  1.4921,\n",
            "        -0.4798,  0.8143,  0.0332,  0.7592, -1.0185,  0.2711,  0.5969, -1.0662,\n",
            "        -0.2800, -1.1044,  1.6881, -0.8265, -0.4197, -1.2552,  0.2325, -0.7864,\n",
            "         0.3675,  0.8907,  0.5122,  0.4087,  0.7174, -0.1609, -0.0686, -1.7964,\n",
            "         0.3659, -0.6827,  1.6251, -1.3716,  1.4374, -0.6759,  1.6262, -0.0206,\n",
            "        -1.3218, -0.2705, -0.9273,  0.6876, -0.9418,  0.3036,  0.3347, -0.7301,\n",
            "         0.9914, -0.3879, -1.2416,  0.2149, -0.0708,  0.3219, -1.2216, -1.0792,\n",
            "         0.2147,  1.5541, -1.2606, -0.0106, -0.7878,  0.5093, -2.4585, -0.1947,\n",
            "         0.6633,  0.6161,  0.8262,  0.3844,  0.0085,  0.0547,  0.0982,  0.3301],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 위생\n",
            "w2i[word]: 12\n",
            "tensor([ 1.0357,  0.4244, -0.0510,  0.4453,  0.0123, -0.8645, -0.6202,  0.9876,\n",
            "         0.5617, -0.4866,  0.0424, -0.8250,  0.5636, -0.1520,  0.7676,  0.4923,\n",
            "        -0.8278,  0.2651, -0.2938, -0.6869, -0.4167, -0.3580,  0.6971,  0.8686,\n",
            "         0.2510, -1.6132,  0.9145, -0.9420, -2.0348, -0.0276, -0.9275, -0.0516,\n",
            "        -1.0158, -0.1413, -0.2852,  1.2896,  1.3260, -1.7350,  2.0626, -0.9442,\n",
            "        -1.2122,  0.7775,  0.6343, -1.4429, -0.5240, -1.7591, -1.3169, -1.4248,\n",
            "        -1.3833,  0.3048,  0.8384,  0.2678, -0.9865, -0.5631,  0.2496, -1.1296,\n",
            "         1.3036, -0.1895,  0.1511,  0.3489,  0.6267, -1.5417,  0.2640, -0.2466,\n",
            "         0.4734,  0.1152, -0.1195, -0.6551,  0.0698, -0.8526, -0.9366, -0.9153,\n",
            "         1.7815,  1.3741, -0.5797, -0.2137,  0.3393,  1.0634, -1.0379,  2.0503,\n",
            "        -1.0958,  0.3416, -0.7206,  1.8285,  0.9117,  1.7767, -0.5610,  1.0779,\n",
            "        -0.2202, -1.1182, -0.7249, -1.4061, -0.7792,  0.3193,  0.7514, -0.1715,\n",
            "        -0.8123,  0.9698, -2.5959, -0.3063, -1.5103,  1.0985, -1.3827, -0.4330,\n",
            "        -0.8334,  1.5824,  0.3485,  0.3781, -0.0700,  0.4858,  0.7832, -1.7517,\n",
            "         1.1761,  0.0308,  0.1641,  0.2663, -1.2813,  0.5753,  0.2246, -1.1739,\n",
            "        -0.3013,  1.1311,  0.2257,  0.4545,  1.2347, -0.3703,  0.1979, -1.5418,\n",
            "        -1.8211, -0.2305, -0.1429, -0.1582, -1.3968,  2.0794, -0.1233, -0.1046,\n",
            "        -0.6566,  0.4764,  0.5312, -1.9938,  0.0027, -0.1438, -0.0883, -0.6348,\n",
            "        -2.3449, -1.0911,  1.1159,  0.8049,  0.0863,  1.3315, -0.5288,  0.6773,\n",
            "         0.2685,  0.3508, -0.4727, -0.1793,  0.0374, -0.8924, -0.8830, -1.3710,\n",
            "        -0.5297, -1.2247, -0.6653,  0.2023, -0.2071,  0.3141, -0.4356,  2.4177,\n",
            "         0.2447, -0.1676,  1.2093,  1.2710, -0.7791, -2.2473, -0.8500,  0.6380,\n",
            "         1.3340,  0.1392, -0.2087,  1.5276,  0.8603, -1.1069, -0.3352, -0.1288,\n",
            "        -1.3785,  0.7050,  0.4695, -0.8846,  0.1072,  0.4679, -0.7569, -0.3358,\n",
            "        -1.0092, -0.5452, -0.4365, -0.2642,  0.3553, -0.4971, -0.4456,  0.5411,\n",
            "         0.1412,  0.7173, -1.7347,  0.5934,  0.4146, -0.0992, -0.5489,  0.9439,\n",
            "         0.0981, -0.2947, -0.9280, -0.1937,  0.4753, -0.3832, -2.5893, -0.1730,\n",
            "         0.8679,  0.6926, -1.9409, -0.4447,  0.1124, -0.8129,  1.5649, -0.1978,\n",
            "         0.1727, -1.0364,  1.4637,  0.2974,  0.2290, -0.2316, -1.7766, -0.4603,\n",
            "        -2.0433, -1.0025,  1.4172,  1.0439,  0.2863,  0.1009, -2.4621,  0.7896,\n",
            "         0.7854, -0.4088, -0.9696, -0.9207,  0.3392, -1.3543,  0.5341, -2.1874,\n",
            "        -0.0278, -0.3798,  1.2332, -0.7850,  0.3329, -0.4934, -0.1434, -0.1588],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 가격\n",
            "w2i[word]: 23\n",
            "tensor([-9.7050e-01, -2.8002e-01, -6.4643e-01, -7.7116e-02,  8.1996e-01,\n",
            "         1.5575e+00, -2.9370e-01, -8.0747e-01,  6.1688e-01,  8.1900e-01,\n",
            "        -2.9781e-01, -6.6288e-01, -1.4966e+00, -1.2222e+00, -1.1154e+00,\n",
            "        -9.1033e-01, -7.0429e-01, -8.3086e-01, -6.5590e-01,  2.5047e-01,\n",
            "        -5.4795e-01, -3.0950e-01,  3.2034e-01,  1.8091e+00, -2.7715e-02,\n",
            "        -9.7692e-03,  9.3760e-01,  1.4056e-03,  6.1432e-01, -9.9250e-01,\n",
            "        -7.4555e-02, -8.2314e-01, -1.3948e+00,  1.7872e+00, -7.5528e-02,\n",
            "         6.7309e-01,  1.0702e+00, -5.7262e-01, -1.3152e+00, -6.5864e-01,\n",
            "        -5.7056e-02,  2.5670e-01, -3.8637e-01, -7.9443e-01, -1.4484e+00,\n",
            "        -4.7232e-01,  4.0664e-01, -2.1820e+00,  6.0789e-01,  8.0372e-01,\n",
            "        -7.8511e-01, -1.4602e-01,  1.2623e+00,  1.7315e+00,  8.3401e-01,\n",
            "        -1.7187e-01,  1.5730e+00, -5.1103e-01, -3.8541e-01,  8.7527e-01,\n",
            "        -6.3356e-01,  1.0651e+00, -6.3281e-01,  7.1162e-01, -2.5136e-01,\n",
            "        -7.2142e-01, -3.7538e-01,  1.0510e+00,  1.1851e+00, -1.9989e+00,\n",
            "         2.0751e+00, -7.3423e-01, -6.4635e-01,  9.0963e-01, -1.7816e+00,\n",
            "        -8.7087e-01, -2.4567e-01, -1.0273e+00, -7.4820e-01, -1.2218e+00,\n",
            "         1.7984e+00, -1.3872e+00, -4.5802e-01, -4.4650e-01, -3.6750e-01,\n",
            "         6.3925e-01, -8.6554e-01,  1.1712e+00, -1.4404e+00, -1.2104e-01,\n",
            "         1.2135e+00,  1.2795e+00, -1.3202e+00, -5.2897e-01, -9.4858e-01,\n",
            "         1.0833e+00,  5.4829e-01,  1.1501e+00,  1.2206e+00,  7.3993e-01,\n",
            "        -2.3908e-01, -8.1220e-01,  2.2410e-01,  1.5773e+00,  1.8361e+00,\n",
            "         2.1377e+00, -7.8178e-01,  5.4632e-01, -5.5169e-01,  2.3628e-01,\n",
            "         1.7244e-01,  9.1869e-02, -1.3695e+00,  7.4192e-01,  6.5657e-01,\n",
            "         4.4159e-01, -2.3390e-01,  9.4741e-02, -1.4099e+00,  6.5034e-01,\n",
            "        -1.0969e+00,  6.6058e-02,  8.0795e-01,  1.1289e-01, -1.9806e-01,\n",
            "         2.4787e+00,  4.2168e-01, -7.9593e-01, -1.3183e+00,  1.3270e+00,\n",
            "        -2.4770e-01, -8.6669e-01, -7.2104e-01, -2.3910e-01,  1.1570e+00,\n",
            "         6.5830e-01,  5.3838e-01, -1.7610e+00,  2.7625e-01, -5.0187e-01,\n",
            "        -1.6561e-01,  1.9863e+00, -6.3781e-01,  6.7692e-01, -1.5643e-01,\n",
            "         1.9213e-01,  1.8563e-01,  5.6207e-01,  3.1207e-01, -1.1698e+00,\n",
            "         6.9996e-01, -4.6659e-01, -1.1586e+00, -4.4262e-01,  7.6480e-01,\n",
            "         9.3647e-01,  1.4970e+00,  3.9710e-01,  1.1621e+00,  4.9034e-01,\n",
            "         8.3243e-02, -9.0646e-02, -8.6339e-01, -3.1563e-01,  4.6091e-01,\n",
            "         5.4472e-01,  1.3349e+00, -5.1718e-01,  4.1373e-02, -8.1488e-01,\n",
            "         2.7134e-01, -3.1518e-02,  6.8669e-01, -3.7501e-01, -1.2419e+00,\n",
            "         6.5078e-01, -7.6386e-01,  1.8934e-01,  6.8598e-01,  3.5347e-01,\n",
            "         6.3544e-01,  9.0973e-01, -1.1600e+00, -5.9880e-02, -4.6926e-01,\n",
            "         1.1520e+00, -2.7271e-01, -3.0932e-01, -9.0881e-03, -3.6088e-01,\n",
            "        -1.0589e+00, -8.0398e-01, -1.2365e-01,  1.1631e+00, -4.2336e-01,\n",
            "         5.0742e-01,  1.4145e+00,  8.0522e-01, -2.0365e-01, -1.1414e+00,\n",
            "        -1.9502e+00,  2.1773e+00, -9.2069e-01, -6.4999e-01,  3.6299e-01,\n",
            "         1.0877e+00, -1.1027e+00, -1.3163e+00,  1.0854e+00, -6.2898e-01,\n",
            "        -8.7830e-01,  7.3825e-01, -5.8086e-01,  1.1057e+00,  3.6988e-01,\n",
            "        -1.7961e+00,  6.7490e-02,  7.2404e-01, -1.0392e+00,  3.6565e-02,\n",
            "        -5.4026e-01, -1.3973e+00,  1.9384e+00, -1.9622e+00,  1.4123e-01,\n",
            "         2.8415e-01, -7.3011e-01, -1.0999e+00,  2.9607e-01, -4.6503e-01,\n",
            "         1.0882e+00, -1.0488e+00,  9.0690e-01,  1.9870e-01, -2.2221e-01,\n",
            "         4.4785e-01, -1.2050e-01,  1.8439e+00,  4.5772e-01,  1.9021e-01,\n",
            "        -1.4857e+00,  1.5461e+00,  5.9085e-02, -8.1975e-01,  6.6363e-01,\n",
            "        -1.2125e+00, -1.1926e-02, -9.6034e-01, -2.8050e-01,  2.5937e-01,\n",
            "         1.0905e+00, -1.2065e-01, -2.3381e-01, -2.2373e+00,  7.7404e-01,\n",
            "        -1.4200e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "outputId": "d7fbdb58-c248-4e5f-8bf9-65f45d280716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = skipgram.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(max(emb.squeeze(0)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor(3.1520, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 맛\n",
            "tensor(3.5062, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 서비스\n",
            "tensor(2.3904, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 위생\n",
            "tensor(3.4639, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 가격\n",
            "tensor(2.8429, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4mv-fDF29Ha",
        "outputId": "7be6ba4a-bf9f-4bdb-acde-d8aef0261cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_words"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['음식', '맛', '서비스', '위생', '가격']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jYY7xYd4vIR",
        "outputId": "a0bb301b-c672-46b2-cd59-a7e28cba86d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "i2w[25]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'다시'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OBlu8O63CXx"
      },
      "source": [
        "def most_similar(word,top_k=5):\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  input_emb = skipgram.embedding(input_id)\n",
        "  score=torch.matmul(input_emb,skipgram.embedding.weight.transpose(1,0)).view(-1)\n",
        "\n",
        "  _,top_k_ids=torch.topk(score,top_k)\n",
        "\n",
        "  return [i2w[word_id.item()] for word_id in top_k_ids][1:]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c75HQoLn2_Ty",
        "outputId": "f23f4e01-c1eb-473c-9e14-191caffe0e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "most_similar(\"가격\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['개선', '에', '너무', '싶다']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvYEGNr2IiR"
      },
      "source": [
        "## Word2Vec 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBVrabH2IiR"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XdtYVf8Ydg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#matplotlib 패키지 한글 깨짐 처리 시작\n",
        "# plt.rc('font', family='NanumMyeongjo') \n",
        "plt.rc('font', family='AppleGothic') #맥"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEypFpsw7K8q"
      },
      "source": [
        "pca=PCA(n_components=2)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NAllD3y7NFo"
      },
      "source": [
        "pc_weight=pca.fit_transform(skipgram.embedding.weight.data.cpu().numpy())"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KKgYYTa7Uh3",
        "outputId": "82318e6d-734a-49e1-da0e-3417b4ff871d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for word_id,(x_coordinate,y_coordinate) in enumerate(pc_weight):\n",
        "  plt.scatter(x_coordinate,y_coordinate,color=\"blue\")\n",
        "  plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX4adG7B7qdV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}